Defining Behavior for Interaction Design

Interaction design isn't about the behavior of the interface or a device;
it's about the behavior of people.
It's not just about motions, transitions, and animation. These are helpful for
attracting attention, making associations, and providing structure, but once
we have their attention, how do we help them complete their tasks, or solve their problems?
What should they do?
First, let's define what we mean by interactions.
We're referring to what people are doing while using a device or interface.
We need to use an operational definition, that is, a definition based on what
we're able to objectively observe and measure.
We can operationally define interactions in terms of what people do, such as
click, tap, and gesture.
Our tools or devices like phones, tablets, and gaming devices, and since these
are physical devices, our work may include ergonomics, human factors,
industrial design, and technology to expand and broaden the design process
beyond the digital interface.
However, digital interfaces are really about interacting with information.
Creating, modifying, and understanding are all cognitive processes,
but these are difficult to observe and measure the way we could count clicks or taps.
It's more difficult for us to define thinking about information, or extracting
meaning from data as an observable behavior, because we cannot see these mental
processes, but we can talk to people about them.
We can observe for physical behaviors, and we can ask about cognitive processes,
or sometimes infer those processes based on our observations.
For example, when we see someone using the Back button repeatedly, we can infer
that they're lost or uncertain, but we should confirm that by asking.
Designing for behavior is more than just knowing where to place a button, or
how to make it look clickable, and since we've already discussed many of these
mental processes, let's focus on crafting interfaces that facilitate interactions.
Let's design for behavior.

Perceived Affordances

How do we know where to interact?
Interfaces include both static information, and opportunities for interaction.
Not everything is interactive,
though in the digital world, it could be.
When using this travel site, hipmunk, when we first look at it, we see some clear
opportunities for interaction, because they look like buttons and tabs.
But as we mouse around, we find additional interaction opportunities, because on
rollover, information appears.
In fact, we can even filter our search by dragging these bars on the sides.
What we may think is just plain, read-only information is actually part of the
interactive experience.
We need to provide perceivable and noticeable cues for people to invite them to interact.
Our understanding of how much of the physical world works seems to be innate.
James Gibson used the term affordances to describe action possibilities that are
latent in the environment.
In other words, the physical characteristics of objects make possible what we're
able to do, and we recognize when we're able to interact with that opportunity.
Affordances are always relative to the individual, and their ability to interact.
For example, on a sound mixer, the knobs look like they can be turned,
buttons look like they can be pushed, and glides look like they can be moved along tracks.
Affordances don't require conscious attention and thought, and we might not even
recognize when our behavior is based on an affordance.
Affordances seem to arise out of an intrinsic understanding of our physical
relationship with the world around us.
For example, when hiking, we may choose an uphill path that minimizes the effort
necessary, without even realizing that we have chosen a path that optimizes
distance, time, and energy.
Digital interfaces, however, are not real physical objects;
they're images, symbols, or representations of reality.
A button on a Web page only resembles a physical button; it can't really be
pressed, it doesn't really move, and it doesn't provide the same tactile
and auditory feedback.
A button on a Web page is just a projection; it's light on a screen,
yet we still understand that this digital button presents an opportunity to
interact, because the appearance of the button relies on a perceived
affordance: it looks like a button, so we interact with it as if it were a real, physical button.
Donald Norman introduced the concept of the perceived affordance.
When people perceive a similarity between a digital representation, and an
actual physical object, they understand that they may interact with this
digital image in a similar way.
This is a perceived affordance.
A button on a Web site often has the appearance of depth, and looks like it can be
pushed down, just as a physical button can.
The physical button has an affordance, and the Web button, which is just an image
made of light, has a perceived affordance.
We can take advantage of affordances by creating interface objects that look
like something with which we can interact.
It might be an obvious and analogous relationship, like buttons and sliders, but
it can also be more subtle, like textures.
Since real world objects are three-dimensional, we can use common depth cues
such as shadow, perspective, overlap, and blur.
Textures invite interaction, because they look like something we could feel, like
water, or grass, or sand.
We treat the mouse cursor like an extension of our hand or finger, and we will
often click or tap on things that have a textured appearance.
For example, the thumb on a scrollbar typically has a few marks to indicate a grip zone;
a texture we could feel with our fingertip on a real sliding button.
The Microsoft surface lagoon is a successful demonstration, where a tabletop
looks like water in a stream running over smooth rocks and pebbles.
The image of the water has a perceived affordance for touch.
This perceived affordance invites interaction, and people freely dip their
fingers into the stream, and the water reacts as real water would, with ripples
that flow around their fingers.
It's just an image of water, a projection of light, and yet people behave the
same way they would if it were a real stream: they touch it.
So affordances arise from physical properties of real world objects, and
perceived affordances arise when images have a similar appearance.
If it looks like something we could interact with, if we have a mental model for
the interaction, if the context is correct for interaction, and if we are able to
interact with it, then we do.
For example, if I'm listening to music, my context, and want to lower the
volume, my need, I will look for a knob, or a sliding control: an expectation
from my mental model.
But if the digital interface controls volume with checkboxes, the conceptual
model, I may fail at my task, or at least will be slowed down by cognitive
friction while I try to find and understand how to control the volume.
The experience will be confusing, and less enjoyable.
But why do people press buttons on interfaces, even when those buttons lack
depth cues, or barely resemble buttons?
Because we learn from our experience. We transfer what we learn in one situation
to other similar situations.
We generalize what we've learned.
If it looks enough like a button, we'll interact with it as if it were a button,
even if the perceived affordances are weak.
If you're designing an interaction that's new and unfamiliar, it would be
more important to leverage perceived affordances, and maybe even explicit instructions.
When drag-and-drop interfaces first appeared on Web sites, we had to use
explicit instructions, and provide cues about what could be dragged, and where
it could be dropped.
But now that drag-and-drop is more familiar, and people have learned, we have
simplified the experience with fewer instructions and cues.
On touch devices, people assume that drag-and-drop is available, because moving
icons with our fingers is even more like the real world.
This is direct action, and we'll discuss it soon.
If you're designing an interaction that is very familiar and common, you have
more flexibility, and may be able to design an interface with fewer perceived
affordances, because people will already understand the interface, and apply
their past experiences to it.

Inputs and Sensors

In the early days of computers, information was input with physical switches,
papertape, and punchcards.
Eventually, we moved onto keyboards and mice, which are still some of the most
common ways we interact with interfaces today.
Modern devices have moved beyond keyboards, mice, and touchpads to include
touchscreens, spatial gestures, and voice.
Data entry maybe explicit -- that is, we actively enter the information, and
interact with the interface -- or implicit -- that is, sensors in the devices
automatically detect and record information, often with little or no
interaction on our part.
Explicit interactions include typing, gesturing, and speaking.
We choose or identify the information we want to add or modify, and then we take
the time to enter it.
We type e-mails, dictate text messages, take photos to upload or share, and even
use gestures to play games.
Implicit, or automatic data entry includes light sensors, GPS, and compasses,
microphones, accelerometers, and other ways to detect and record information in
the world around us.
Light sensors are used to automatically adjust the brightness of a screen, GPS
can identify our location and direction, and accelerometers detect the movement
and rotation of the device.
We don't need to tell our tablet computer to rotate the view of the screen;
it does so automatically when we rotate the device, because it senses the
movement and direction.
Many interactions exist in a gray area between explicit and implicit.
Digital cameras use face detection algorithms to identify people in the
frame, and focus on them.
We aim the camera, and it knows where to focus.
The camera is the sensor.
The explicit interaction relies on the person to choose the photographic
subject, and say when to take the picture, and the implicit action uses
algorithms to focus on it.
Digital devices can make our interactions easier by automatically performing
some of the steps in the task.
The device becomes an extension of us.
We need to understand what people want to control, and what they want to
have done for them.
But we also need to identify opportunities to do things for people by
anticipating their needs, knowing what information is necessary to meet those
needs, and being able to automatically collect some of that information,
rather than ask for it.
We can design smarter, more enjoyable experiences by doing the work that people
don't need or want to do.
For example, GPS navigation devices are very popular, because all a person needs
to do is tell it where they want to go.
The device detects their current location, and their direction of movement, then
calculates the best route to their desired destination, and provides
step by step directions.
If the person misses a turn, the device automatically recalculates the route, and
provides new directions.
The person never has to request the next step, or a correction; once the device
knows the destination, everything else is automatic.
This reduces the cognitive load of the navigation task, provides structure and
predictability, and tolerates errors.
We can use mental models to help guide our decisions about how to reduce effort
and cognitive load by reducing the amount of information we need to ask for.
What information, or inputs, must be provided by the person, and how will they enter it?
What information could be automatically gathered?
How and where will information and feedback be displayed for the person?
We can then design for specific behaviors by leveraging device sensors,
contexts, and needs.
A popular music service for mobile phones listens to songs, and tells the person
the song artist, and title, and even offers to help them buy it.
The interaction is simple;
the person activates the application on their phone, and presses a Listen button.
The application uses the phone's microphone to sample sound from the
environment, and sends it to a server where it's analyzed and compared against a
huge catalogue of music, and when a match is found, the information is sent
back to the phone, and displayed as text and images; the song title, artist
name, and the CD cover.
The application uses implicit, or automatic sensing to detect sound.
The person doesn't even need to tell the application when to stop listening.
When enough sound has been sampled, the application automatically stops.
The entire process closely matches our mental model for identifying
and purchasing a song.
Listen, think, recognize, and buy.
The application performs a complex task for us, and the entire interaction is
simple, and nearly effortless.
So it's not necessary to ask people for everything.
We can reduce effort by using sensors to automatically gather information that
can help us design smarter experiences to do more of the work. Then we only
need to stop and ask people for information when we can't gather or find it
another way.

Designing for Clicks and Taps

Although touchscreens are popular, and have become quite familiar, the mouse and
touchpad are still some of the most common ways to interact with an interface,
especially for desktop and laptop computers.
Even though they are different input methods, they do share some similarities.
There are best practices that apply to both touch and mouse, with just a few
adjustments for each.
Using a mouse to control a cursor, or tapping on a screen, is called a
ballistic movement.
A ballistic movement starts with a general direction, accelerates toward the
target, slows down and refines the aim as it approaches the target, and finally,
acquires, or hits the target if it's accurate.
If we approach a target too quickly, we're likely to miss and overshoot.
If we don't accelerate enough, it may take too long to reach the target, or we
may fall short of it.
This type of movement has been studied and described mathematically.
Both Fitts' Law and Meyer's Law can be used to calculate the time to reach a
target, as a function of the distance to the target, and the size of the target.
Simply put, it's easier to successfully hit targets that are close and large
than to hit targets that are far and small.
If you're concerned about efficiency -- that is, reducing time -- and accuracy -- that
is, reducing errors -- then the distance to, and the size of a target are
important considerations.
For both mouse and touch, the farther we have to move to reach the target, the
longer it will take.
The easiest target is the one already under the cursor or finger;
we don't need to move at all to hit it.
When using a mouse, there are a few other places where targets are easy to acquire;
the corners and edges of the screen.
Most screens have bounded edges, that is, the cursor stops when it reaches them.
This means that we can rapidly move or fling the cursor toward a target in a
corner, or along an edge, and not worry as much about slowing down and refining
our aim, because the edges will either funnel the cursor into the corner for us,
or the cursor will stop automatically when it reaches the edge.
As long as our initial direction is accurate, we can very quickly reach the target.
Edges and corners are not as beneficial for touchscreens, though, because our
fingers can move beyond and off the screen,
we can still miss those targets.
So how do we take advantage of the best target location?
Although, we can't move buttons around the screen to match the cursor location,
we can use contextual menus, tool tips, pop-ups, and heads-up displays to
present information and functionality right where the cursor is.
We can open menus and dialogs near the cursor, so that the person doesn't have
to move the cursor, or their finger, very far in order to make a choice, and we
can make these layers draggable, so people can move them out of the way when necessary.
Size is also a factor;
larger targets are easier to acquire, but since we have limited space on the
screen, we need to balance the size of the interactive elements with other
content and images on the screen.
Operating systems and electronic devices all have human interface guidelines
to set the standards for things like the size and placement of buttons, tabs, and icons.
These standards ensure consistency and usability, and if you're designing for
native applications, such as iOS, Windows, or Android, make certain you've read
the guidelines for that system.
Calculating target size is complicated by two things;
pixel density, and physical dimensions.
As screens become smaller, and higher resolution, the number of pixels increases.
So a button that is 30 pixels square on a 1024 by 768 resolution monitor will
need to be 44 pixels square on a 1600 by 1200 resolution monitor just to look the same size.
And for touchscreens, we need to consider the physical size of the human finger;
it's not measured in pixels,
so we need to be able to convert the physical size of a finger into the pixel
dimensions of a touch target on the screen.
When the screen resolution and size change, we may need to recalculate the
size of the touch target.
Just hitting a target isn't always enough.
When using a mouse, some navigational systems on Web sites, and some toolbars on
applications, use nested flyout menus.
As long as the cursor remains over the flyout menu, it remains visible.
But if you need to move the cursor through a narrow channel into a submenu, it
can be difficult to keep the correct, or any menu open.
Keeping a menu or a layer visible based on the cursor location can be challenging.
The more time and effort it takes just to move the cursor slowly for
accuracy, the less time and cognitive effort people have available to think
about the information, make correct choices, make correct decisions, and
enjoy the interaction.
Finally, we need to consider timing for the appearance of things that appear
when the cursor moves over a target.
Just because the cursor is near or over a target does not mean that the person
wants or needs to see a new layer appear.
They may simply be moving their cursor to another part of the screen, and
unwanted layers, motions, and transitions that occur while they are moving the
cursor are an unwanted distraction.
Let's take a look at how a simple delay can prevent a navigation menu from
opening when it's not necessary.
As we move the cursor quickly across this navigation, it doesn't react to us.
However, if I want to see what's in the navigation, I need only pause for a moment.
This is convenient because if I'm moving the cursor from the body of the page to
a link at the top of the page, the navigation system doesn't react, but when I
need it, it appears quickly.
If the delay before displaying something is 0 milliseconds, then any time the
cursor appears the hot zone, the interface will react instantly.
The simplest way to reduce the number of unwanted displays is to use a brief delay.
For example, a delay of just 150 to 250 milliseconds before the interface reacts
will help people moving the cursor over the hot zone to get to another part of
the screen without triggering the interaction.
But if someone wants to hit that target, then they will pause the cursor in the hot zone.
The delay will elapse quickly, and the reaction will occur.
We could also use a more elaborate system called hover intent, where we measure
the velocity, acceleration, and deceleration of the cursor.
If the velocity is constant, or the cursor speed is accelerating, then we can
infer that the person is moving over the hot zone to go somewhere else.
However, if the cursor is decelerating, and velocity approaches 0 over the hot
zone, then we can infer that the person wants to hit the target,
so we activate the display.
Finally, remember that there is no hover on touchscreens,
so touch interfaces need to be designed to rely more on explicit tapping and gestures.
When designing for taps and clicks, we have the opportunity to reduce effort by
making certain that things are easy to reach, by bringing content and
functionality closer to the point of interaction, and by ensuring that the
interface only reacts when people expect it, and need it.

Providing Opportunity for Direct Action

As computer technology has evolved, we've moved from a very remote and symbolic
form of interaction, to more direct action.
In the early days, flipping switches to set binary values, and punching holes in
cards, required making decisions, and coding data, and then waiting hours, or even
days, to get the results.
We were very far removed from the information we were trying to understand.
The command-line interface, monitors, and terminals made things faster, and easier
to correct, but we were still working with abstract representations of
information and processes.
The arrival of the graphical user interface, also called GUI, and the mouse
suddenly made it possible to interact more directly with meaningful
representations of information.
We could drag pictures into photo albums, and documents to the printer.
This was more like the real world, but we were still using a mouse out here to
click a button over there to make something else happened somewhere else.
Touch screens are getting us closer to direct action. We can now directly touch
the article we want to read, the photo we want to see, the song we want to hear,
and move solitaire cards with a gesture very similar to playing with real cards.
Touchscreens and gestures are more direct than a mouse, because a mouse is a
translated movement and action, but touch is direct.
We can tap what we want, instead of moving a mouse to move a cursor, then
clicking on what we want.
Drag-and-drop for both touch and mouse is more direct; we don't need to select
an object, then select an action for it.
For example, when I rearrange photos in my Flickr account, I can simply click,
and drag that photo to its new location, and as I move it, you can see that the
grid opens up a place for me to drop that photo.
Every time I move a photo, the grid rearranges itself, and it clearly communicates
to me, this is an open and valid position; an excellent form of feedback.
Layers of content are more direct, because they bring information and
functionality to the person at the moment they need it, without disrupting
context or flow, and they can give people a greater sense of control over the interaction.
Sometimes the layer is displayed on demand, and sometimes the layer is
automatically displayed, based on context.
There are several advantages of direct action; the interface is often visually
simpler, because the interactive objects are represented directly, and other
content or functionality is presented only when needed.
The interaction is easier to learn, because there are no layers of abstraction
between the person and the information.
The interaction is easier to remember, because it often corresponds more
closely to our real world experiences.
People make fewer errors, because the interaction is a closer fit to our mental
models, and expectations.
It encourages exploration; when people can interact with almost everything, and
have relevant content and functionality brought to them at that moment, then
they become discoverers, and they ask, oh, what can I do here? What will happen
if I do this? Or that?
The experience is more pleasing and satisfying, because it's easy, makes sense,
and is more engaging.
Although direct action isn't always possible, we should strive to not separate
the action from the object.
If you need to select an object in one place, and choose an action in another,
or if the interaction does not map closely to our mental models, and feels like
there are extra steps, then there are opportunities to simplify, and to be more direct.
Take the time to critically review the interaction design, and the flow of the experience,
because we sometimes make design choices based on the technology, rather than the
person, and when we're designing for the efficiency of the machine, we fail to
consider the expectations and experiences of the person.
Direct action is becoming more important, and more available, because
touchscreens and spatial gestures are changing the way people interact with
information on their devices.


