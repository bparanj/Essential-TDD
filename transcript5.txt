[Continue video 3 ("TDD bootcamp part 2") at 1:45:10]

S: Explanation test. This is actually-- everybody is in a situation where I want people to adopt TDD, and even though other developers don’t have that discipline to write the test first, another developer comes to me and starts talking to me about how the system should behave, I give the explanation in terms of tests. “Oh, given this setup, if I do this exercise, this is what I expect to happen.” Given then when, I actually explain them in terms of tests. “Is this what you mean? Yeah, yeah, yeah, and then I write a test. Explanation test. You may say something to me, Well, I’m talking to you, but when I go and actually get the test to work, that assumption may not be true. Only the test can tell me whether you are right or wrong or maybe you just forgot to mention one minor detail because it’s kind of like vague. There can be no vagueness in tests. There’s only pass or fail, no maybe. It has to be clear and precise. And the test cannot lie. Sometimes there is false positive, false negative, that means it’s lying. 

So another test. So this is something like for instance, you’re writing a controller-- some logic is getting messy and-- should be thin controller fat model. You know what? We should actually move this thing to the model layer. Let’s write another test at the model level and then we’ll come back to the controller. Write another test. Don’t get distracted--  put a pending in your controller, pending this, you move the logic into your model test, make that one pass, and then come back to your controller. You can remove the pending and then get this thing working using the new API that’s in your fat model. Sometimes it’s not related. Something is distracting you which is not related, you just put it in your to-do bucket. You’re having a pair programming session, and suddenly you say what happens if this, this, and all that, and you’re just trying to divert my attention. If it’s not related, I put it into my queue. “Oh, that’s a very good point you brought in. I think it will have business value. Let’s put it into our to-do list. It should do something blah blah blah, pending. We’ll come back to it later.” So you go back and focus on what you’re working on right now. It’s another test. Later, pending. 

So the regression test. When a bug comes in, you actually, ideally are supposed to write a test that’s going to expose the bug. Okay, the bug exists, and you fix it, so the failing test now passes, so now you know your bug has been fixed. It hasn’t broken anything. And then the question that you ask-- that’s the regression test you wrote to fix the bug. And then the question that you ask yourself is, Where else in our system can a similar thing have happened? Where else can a similar thing have happened? You’re basically trying to see my safety net, the holes in my safety net; I want to make it smaller. So I don’t get more regression bugs going through. Regression test just means you’re not-- system is telling you you’re not done designing me yet. You haven’t sufficiently had enough coverage to be confident, Okay, this is going to work and provide an business value.

A: What is the explanation test?

S: Explanation test is basically you’re in an organization where TDD is not really the thing, and you’re asking something-- they’re asking something about the system or you’re asking for clarification from them. But the way you communicate is using the structure I showed you, the four step. Given when then, or setup, exercise, verify. Oh, is it how this thing is supposed to work? You capture that so that you can write a test for that. You get to the-- developer conversation, you get to that structure, so it’s kind of like you’re learning more about that system, maybe you’re new to the company, and this thing gives you a framework to talk in a very structured way and also make sure whatever the information they’re providing you is the right information. 

Use TDD to write test utility, like you said, the way you’re checking what you’re doing, I can just have an assertion for custom range check or something; I could name it domain-specific way, but you need to have a test to test that so that it can work across different data sets, catching the boundary conditions, and all that. 

So use TDD to write test utility so others can use it. Because I want confidence; I want to make sure there is no bug in the test, or the library that I’m going to use across all of my tests. 
So, this just kind of shows at the setup level, you can have a creation method, it’s like a factory method. If you’re using factory girl, that is basically the creation method right there. You don’t have to write your own utility. And then finder method, you won’t really have much finder method. Exercise phase, encapsulation method, this is when you’re actually exercising the behavior, sending something. Verification stage, you have verification. These are all like customization in the verification stage. Cleanup method, if you want to clean up resources, if it’s very specific to paypal thing, you want to have a cleanup method that you can reuse across all your integration tests for instance. So each of these things raises the level of abstraction. Depending on what step it is, it’s just named differently. Test utility methods. And you have tests to actually test these methods, your domain-specific different tests that you’re going to reuse across different modules, classes, that you might retest. 

So don’t have any loops, keep the tests simple, no if statements. If, but, all, else, no switch. No each, for each, all those things. Why do we say you don’t really need to have loops, conditionals, all that? More conditionals you have, more code paths you’re going to be having in the tests. And more difficult it becomes to reason how your test is actually going to behave at run-time. You don’t want that. You want it to be able to just by looking at the source code, immediately you need to be able to say, Oh, okay, I can exactly say what this code is going to do, it’s so simple. 

Avoid conditional test logic, we saw all that. Expected behavior specification, you can actually-- if you have two objects like we kind of discussed this thing, you can pass those two objects because we already know we want to aim for just one condition, verification of just one condition-- what if we need to have a customer-- I want to assert, Okay, the customer was charged. Amount paid was this thing, this was the amount, all those things. If they want to have multiple assertions, I don’t want to do all that. I want to just say, here are the customer object, do whatever assertions you want. It’s like the internal library utility will do all that for you. There’s the expected behavior, specification, you can write your own stuff. It just takes the object and it asserts on different things that you really care about. These built-in assertions-- RSpec has a lot of built-in matchers, Delta assertions, and boolean thing and all that. I will give you a very quick tour of all the RSpec related built-in assertions, more like a crash course. We’ll breeze through them toward the end of it.

A: Yeah, actually I want to do sometimes if you have to create-- I have seen some people using as null objects and third as null objects .

S: Oh, yeah. Remind me as_null_object when I finish that thing; I’ll tell you when you use it and why you use it. Domain assertion really specific. Your business logic is a domain-specific assertion.  It raises again, becomes more expressive. Sometimes you start to have puts here, puts there, and you want to see what’s going on, what the expected value, what’s happening, instead of using puts, just write your own diagnostic assertion that’s going to say, okay, this was expected and this is what I got. It’s very very self-explanatory. When you run this assertion, you will still have a puts, but it’s basically assertion and it’s more-- it gives you more diagnostic information. Instead of having puts everywhere, you-

A: Don’t put puts-- to assertion-
S: Exactly. Put diagnostic assertion so it provides you-- if you need more information-- see, if you’re using a debugger, the main reason for all this TDD is don’t spend like three or four days trying to hunt that one elusive bug. You don’t want to spend your time in the debugger. But putting puts is kind of like substitute for not being able to inspect variables or looking at that particular attribute, value, stuff like that. Instead of doing that, create a diagnostic assertion, and use that diagnostic assertion which will show you what are all the values of that particular object are interested in. You can have a custom to_s method for that custom object, and you can say, the customer first name and last name has this, this, this, something to_s which will make sense when it is output to the standard out. So you will know, oh, okay, this customer was charged twice or just by looking at the output, you can get a lot of clarity on what’s going on. 

A: So basically I was debugging nasty code with a lot of logical branches, like if conditions, and the code-- [the input SS]-- is not taking the path I expect it to go, so I had to put a lot of puts throughout-
S: Here’s the thing. Especially if it’s a code that you have not written, somebody else has already written the code, the only way you can learn about the code is by putting the put as statement in there and running the browser, checking to see, okay, this is-
A: No, this is my code. It turns out because the input object wasn’t the right class. So you’re saying instead of using puts I could just write assertion statement in my TDD test case to tell me what’s the result of this evaluation. 

S: You can write an assertion, custom utility for your object, that can take that particular object, or you can write even a general one, as long as your object can respond to to_s and dumps all the attributes for that particular object.

A: Essentially writing was in my if statement and then sort of assert whether it’s true or false, like turns out to be true, or turns out as false.
S: Yeah, you can have a guard assertion instead of if statement. 
A: How do we do that?
A: Let’s say in her case, she wants to check the test. The object is not right. So you’re saying-
A: [I explained it SS] was in hash, but it turned out I was in string.
S: Yeah, that’s a good one.
A: So if object is an instance like this, can we check like that?
S: No. Basically when you start checking toward instances of, instance_of kind of thing, you’re going to lose the benefit of polymorphism. As long as certain thing responds to certain message, I don’t really care if it’s a had, or [1:58:14] because they are responding each [for instance SS] and they use the values. And he’s-- for your case, your assumption was something, was supposed to be hash and turns out to be a string. Basically you’re still debugging. What I would suggest-
A: But you’re saying diagnostic assertion for debugging.
S: Diagnostic assertion can be used for debugging in cases where you want to dump the particular object and you don’t want to inspect the variables-- what are the values in the variables?  I want to see. Why do you do puts ? You want to see.

A: Instead of doing puts-- he’s saying just how we put debuggers, he’s saying just do in split and that way get to what the object does. He’s saying customize the tools method and tools method will print all the way-

S: Print in a very legible way.
A: You’re saying [1:59:15] instead of a code.
S: Yes.
A: Not using diagnostic assertion or TDD.
S: No, this is in some cases you need to inspect-- do the diagnostic assertion. In your specific code example you gave me, there is a lot of if statements and going all those things, you are testing too many things at once. 

Usually, if you’re writing a test, I showed you like four steps. In the four steps, in unit tests, they came up with just three A’s, three steps. So if your test is going beyond three, four, five lines, if you follow one of the test guidelines, it says verify only one condition. Only have one assertion. You only have few objects you need. The minimal object-- you always create minimal objects you need to test your system. Context should be minimal, because you don’t want to create too many objects which is unnecessary for whatever you’re testing, because you want to reduce the test context so your test will be faster. If you do that, your test will be four, five lines at most. If you have a longer test, you’re testing too many things at once. SUT can be too large. That’s one of the beginner’s mistakes again. If your SUT if too large, you’re testing too many things at once, it’s called as an eager test. It’s like a poetry that just keeps going on and on verifying this, that, and every other kitchen sink under the sun.

A: Let’s say I have a suite or four levels of nesting if statements. Can I just break each level into a single test case?
S: Here is a question for you. Can you write tests without writing an if statement? Can you challenge yourself without using an if statement? I have a constraint for you.
A: No, that’s not my code has if statements-
S: You have nested four if statements, right? Whenever you are using if statements, most likely you’re not utilizing the polymorphism to the full effect. You’re depending on something. Like she said, instance of, you’re checking instance of. What is the conditional that you need to have so much of in our nest ifs first of all? Maybe the object is not following the single responsibility principle, for instance. It should do one thing and one thing only.

A: Actually, in this case, it’s not really object. It could be a user input text where the type of message-- based on the type of message, I look at the status. Is it fail or is it successful? I take different kind of actions. It’s not really-
A: I think there might be a way to refactor the code to reduce the level of if statements, but there are different techniques for that. 
S: Yes. The command, like command pattern for instance.
A: If I’m hearing you correctly, you would probably recommend having one test method for each of those different-
S: Yes, I would. Break everything inside of those-- I would break it and separate it on its own.
A: Right, so each level is its separate-- that’s what I was asking. 
S: Yes. Exactly. The one thing I was kind of confused was you were mapping whatever the user is [specifying SS] to some other command or something. Mapping of that-- also you can use polymorphism for that. 
A: In this case it’s not a real object because then I’m getting user’s SMS message, and it has a specific format. So it depends on the message; the message has a few fields. It depends on the value of each field I need to process it differently. Because it’s not really an object-
S: It’s more like a mapping of events to a handler, event handling.
A: Right.
A: You might be able to abstract it to a factor that gives you the handler, and then all the ifs are factored into the behavior separate, but that’s kind of more OO stuff-- I’m just imagining-
A: Are you talking about TDD or are you talking about something else?
A: No, this is something else.
S: TDD and design-- you cannot talk one without the other. If he’s talking about design, yes, he’s on the right path. I totally agree with you. Your code is too procedural. When you’re using TDD, the first goal is to drive the design of the code. This is what I was saying:  Listen to the test. If your test has to be like this, there is a pain there. If it’s going procedural I am saying no if, no loop, I am imposing lot of constraints here. Why?  I want to make the test very simple and the pain is here. But it’s giving you an indication that the design needs to be improved. The human handler mechanism, the way you handle it, you map to the behavior, publish events, somebody subscribes to the event and they add handling to it their own way. Behavior changes based on the environment. It all sounds like polymorphism to me, more polymorphic, dynamic kind of messaging. You have to isolate those behavior, encapsulate behavior in an interface. Because you cannot just move that kind of procedural code in the test. It’s giving you feedback that-- you didn’t think about the design. Think more about the design. I would actually read the Erik Gamma’s Design Patterns book and some of the things that they talk about, you can actually find the context and the scenario and see if your problem actually fits one of the things that they’re talking about. And you can follow how they go about-- yeah, Design Patterns.
A: This code is very low-level code.
S: Oh, you’re talking about low-level code.
A: Very low-level that processes SMS messages that I’m getting-- I need to do some basic processing.
S: Basic processing, I don’t know, maybe it’s too low-level-
A: Therefore it has a few nested if statements because I need to-

S: Few nested, yeah, it’s okay. To me, initially it looked like you were getting different events and you were handling it in a different way, the behavior changes. 

A: But if I’m understanding, you would recommend having different tests for different paths. Your tests are having different code paths in them.

S: Yes, that is totally true, because I really want to nail this point-- because the reason you don’t want to have different code paths in the test is, whenever I run a test, it has to be repeatable. I need to deterministically say, this is the path the code took, I run it 100 times, I need to say, this is going to go through this path, and it goes through this path. If it’s going to go through different path, you need a different test.

A: I understand about the test part.

S: That’s fine, if it’s too low-level, maybe the messaging isn’t going to be applicable to that thing. 
So the verification method you see we isolate, we encapsulate all of the verification, logic, specific to our own thing, our own utility, that’s the verification method. Custom assertion tests:  if you write your own custom assertion, make sure you have a test for that, unless you have a test, you cannot be confident you can reuse across all your other tests. 

So, do over. Sometimes what happens? Sometimes things are going right; sometimes everybody gets stuck. We all get stuck. Nothing is working today. But no matter what you’re trying it’s not working, so what do you do? Throw away the code and start over. That’s what do over is. Start fresh. Take a 5, 10 minute break, and then start fresh. Maybe change your pairing partner. Get a fresh pair of eyes to look through it in a different way and see if you can make progress.  It’s not like every time it’s going smooth, it’s like some days are very frustrating. It happens. And also, when you’re stuck, what do you do? When you’re just programming by yourself, you’re just stuck. You just cannot work through this and make any progress. What do you do? Break. That’s what the break is. So let’s see how much more we have. 

So here is a question for you guys. How big should my steps be? How do I know how big my steps should be? 

A: Don’t spend too much time in the red area.

S: As long as you’re green, you can keep on taking longer steps. If you’re getting to green, and you’re getting quick feedbacks, you don’t have to keep taking small steps. You can take bigger steps. And if you’re red, it means don’t take too long of steps, start taking smaller steps. 

So what don’t you have to test? Otherwise, how many tests should I have? Should I have like Fibonacci-- should I say I should have five tests, six tests, or fifty tests, sixty tests? How many tests is sufficient? 

A: 100% coverage.

S: Oh, 100% is coverage? Are you happy? I have like four tests and it gives 100% coverage, is it-
A: 80, 20, 30%.
S: Some numbers. Doesn’t make any sense, right? So, the thing is not about how many tests you write or what is your test coverage. You’re 100% test coverage, it’s good to aim for, the thing is how much confidence do you have in your code. It’s about confidence. It’s about facing the fear in-- how comfortable are you about making changes to the existing code to fulfill-- satisfy the new requirements, changes. It’s not about validation. It’s all about specification. Driving the design of your code so there is no one specific number that you can calculate and say this is the number of tests you have written; now we can stop. 

So how much feedback do you need? I always want quick feedback. I don’t want to spend too much time in red. Being green, green, green, I want to have that loop. I want to minimize that loop. I want to get a lot of feedback. Feedback is what allows me to learn-- to experiment. To write different alternative solutions. It also gives me confidence if it’s all green, it gives me more confidence. I can try much bigger things. 

And then-- I saw you delete the test.  Why did you delete the test? When would you delete the test? When is it okay to delete a test? 
A: When another cover’s slipped completely or-
S: Yeah. Sometimes you write a test, it doesn’t really test that much, it just kind of gets you going in the right direction. And you learn something, and then you have another test, kind of like you said: it covers the previous one. So the previous test does not have any purpose except for allowing you to learn more about the system. 

Oh, it’s more and more progress for the test. You just delete it. That’s okay, I’m working in a different team and Toefl is working on another team. We have been pair programming and we have been rotating. We ended up writing tests which are very similar. We’re testing the same thing, but we wrote the test in a different way. So, do I want to delete my test, or delete your test? Which test should I delete? Testing the same thing, but it’s written in a different way. The doc string is different, the way we have set up and structured is different. How would I delete this? Since it’s kind of like a duplication, right? Which one should I delete?

A: The lesser one, but-
S: If it is a irrelevant event, if it is like total duplication, if it’s a zero event, it doesn’t matter which test should be preserved. You can delete either one of them. If it’s different enough and it has communication value-- the way he’s emphasizing, the way his doc string is, provides value for his context, for his object, and it provides value for mine, I would just retain them both anyway. 

Because, go back to the goal we had previously. Communication. Communication is one of the values that the test provides. Not just-- this overrides the thing I said before where you should minimize the test overlap. You cannot be repeating the same over and over again. But if this provides communication value to the team members, yes I would retain it. So you need to know what the goals are. You have so many different things that you need to balance and you make a choice. You have to make a judgment based on what the circumstances are for that. 

So, we’ll see what this round trip test is. Round-trip test basically is a test where you use the public API. I’ve been saying use the front door first. This is the front door. Front door is nothing but the public API. In this case, use a real component test, test method one and test method two, are exercising the SUT. SUT is a public API. That’s why I am able to directly call it. And there is a dependent-on component. It collaborates with some other thing-- okay, this is the shopping cart, to be concrete, tax calculator, I want the calculator and then I need to calculate the total charges that needs to be charged to the customer’s credit card. And then I’m retaining that value. That is a round-trip test. It goes one round-trip. It goes input, it returns some value to it. And it uses the public API in round-trip test. That’s a real component test. The fake component test-- you create a fake object. Fake object can be a stub or a mock. 

We’ll see what the differences are pretty soon. We create this, and we install this fake object in the SUT. Which means in the test environment, it’s going to use the fake objects. This is grayed out. Depended-on component is not used the test environment. It will be used only in the production environment. This thing will be. But the fake object is the one which is going to be used. So this is a fake component test. It makes the entire thing run faster. So this is-- calculation of tax calculator, this is a fake tax calculator. It just takes some arbitrary number and just says, okay, 6%. Charge this guy 6% sales tax. That’s what is says. It’s so quick. It doesn’t go to the database, it doesn’t look up anywhere, it doesn’t look up the web service-- internal web service, anything. It’s really fast.

[Stop video 3 ("TDD bootcamp part 2") at 2:15:36]
[Continue video 3 ("TDD bootcamp part 2") at 2:15:36]
S: And now compare and contrast this to the layer crossing test. The first one was round-trip test. Second one is a layer crossing test. The first one you do not have any layers. Now in the layered test, you have layers coming in. Layer one, layer two, layer three, layer n. For this particular layer, I’m only concerned with this layer. I don’t care about this layer. I have a test double there. That replaces the entire layer. The layer beneath is also replaced by a test double. That particular layer is isolated from the layer above and the layer beneath. I’m only worried about that particular layer I’m about. Test concerns separately. Each layer is tested independently. Layer one is tested independently; I replace the one beneath it with a test double, so I have more control over the behavior of those guys. And so on. Layer n the same thing. Above and beneath, they’re all separated. So depending on the component, that’s grayed out, that’s only being used in the production system, not in the test environment. 
Divide and test. Layer by layer. It’s divide and conquer basically. Each layer, you test them separately. Bottommost layer is independent of everyone else. Different from everybody else. Isolated from everybody else. You just separate them like you were talking about, how do I make sure? If you change the contract-- if you divide and test, you will localize the chang,e and you can actually make the test fail closer to the actual cause of the problem, which helps me to localize the defect and fix it quickly. 
How to manage test fixture? We kind of discussed this thing in the morning. For instance, if you are using a factory girl, you have a happy path. For all the happy paths, I have a valid user with a valid first name, last name, email, everything is valid, phone number.  If you’re checking the validation for phone number, you override the added phone number. You override. Your test is clear. That’s how we manage test fixture. It created once, when you need to override them, you override the only specific thing you need. And all of this thing you don’t have to manually worry about these things mainly because if you’re using something like a factory girl, it’s going to take a lot of this burden away from you. You don’t do this thing. You just go by how they recommend you create certain objects here and override them and you just use it. You just know how to use it. That’s it. How to structure a test and use a factory girl API. That’s it. The framework actually worries about whether it needs to create the object, new object, and then replace them, or to update that particular email field alone and just provide the object. It’s all happening behind the scenes. You’re not explicitly as a programmer doing it; you’re not controlling it. 
A: [Inaudible 2:19:28]
A: System crash.  That happens on mine all the time. Mine is PC.
S: So maybe it’s a one-time thing.
That completes the testing guidelines. I just want to take 5 or 10 minute break before we move to the bad test, good test, and start stubs and mocks. That completes the testing guidelines as well as the how to write tests. So we have a few more things to rush through. Maybe we can take just a 5 minute break so we come back at 4:30.
A: [Inaudible] Instead of building everything each time you’re in RSpec, it’s sort of like a cache.
S: Actually, Spork and all that, I wouldn’t use those kinds of libraries. The basic problem is, your tests are slow. How do you deal with this? [Cody Haines SS] has given a talk-- you basically extract all the behavior into your own module and you mix into your own stubs and you test them separately. You make your tests faster. More dependency you have on crazy things like [Spork SS] and all that, too much dependency and a lot of things break. You’re spending time on things you should not be spending time on.
A: Yeah, if you change your routes when you configure, I didn’t realize you had to re-start Spork each time. I didn’t realize why it was failing, and it sent me off on some path.
A: Spork will process in the background and reports back to you.
A: I do find it speeds up-
[Video 3 ("TDD bootcamp part 2") completed at 2:21:45]
[Begin video 4 (“TDD bootcamp part 3”)]
S: So, let’s look at the bad tests, what they’re like. They’re like unrepeatable tests mainly because it’s erratic tests because it has dependencies on external things. External systems, like it has dependencies on system services like clocks, day, time. If you have time-specific things, it’s really unrepeatable. If something is to go out every midnight, it’s based on time. Dependency on time. It’s unrepeatable. So how would you break that dependency if you can only run the test at midnight, for instance? You need, you need to break the dependency. You need to use a virtual clock and fool the system to be thinking, okay, it’s running at midnight at this instant. You need to [have SS] virtual clock, virtual date, so you can have a time travel kind of thing. In Ruby you have a time cop, Jim, that allows you to reset the time to whatever time you want. So that your test becomes repeatable. It’s not dependable on those kind of things. So the interacting test-- if one test fails, leaves the system in a certain state, and the other test depends on the previous test passing so it can continue and work. If you have a test interacting with each other because the test context is being shared across the test. And ideally, if you see the teardown phase, we want to revert back-- [reuse SS] all the resources so that there is no interaction between tests. They are isolated. 
Slow tests, for many reasons. If you have a data set that is too large, then it’s really necessary to specify the behavior. You’re creating too many objects for the test fixture, then you need to create the minimal amount of test fixture, minimal amount of objects created to test your system, otherwise you’re unnecessarily creating all these objects which is going to slow down your system. And also, you need to avoid going over the network and hitting the external systems as much as possible so you can speed up your tests. Isolate your tests from all those external things so you can speed up your test so it can run quickly. You want quicker feedback. Integration tests obviously are going to be slow; there’s not much you can do. You have to make sure you can actually integrate with them. But you’ll run them less often. Not once a night like overnight it’s going to run and it’s going to send you the report which will pass and which will fail. But local tests, I am running as a developer and I want them to be really fast. 
And this assertion roulette is basically sometimes assertion 1 fails, sometimes assertion 2 fails. You have like 4 or 5 different assertions or something, and you don’t even know which assertion is going to fail for which reason because you have all these dependencies on different things. So you always want to have just one assertion fail so that it fails for one reason only so that you can go and isolate. Okay, pinpoint-- this failed because of this particular reason and you can actually go and fix that thing in your code. 
Frequent debugging. Even if you have a test, if you want to frequently debug, there is something wrong with it. We are writing a test to prevent this frequent debugging issue. We don’t want to spend our days in the debugger trying to find the bug. We want to localize the defect and we want to fix the problem quickly. 
Manual intervention, again. Unless you do something manually, you change the configuration or something, the test will not pass. Again, it requires some kind of help from the developer or something in order to do this. Again, it happens because of the dependency. If you have to manually intervene, then something is wrong because it cannot be automated, it cannot be run by itself. It’s not self-checking, basically. 
Conditional test logic, we already saw that. You need to avoid these bad tests.
Complex tests. Tests which is like 15, 20 lines of code, verifies this, that, and everything, but the kitchen sink. Instead of just testing one thing and one thing only, and being simple. 
Complex tests can happen because of the eager tests. You can be verifying like 5, 10 different things, which is [out of element SS], different concerns. General fixture, you’re using general fixture, which means you’re-- you want to reuse the setup code, so you have a general fixture. You have a before each block, but the object you create-- you create a lot of objects there, a huge graph of objects, but not all the tests require them. Some of them only require a few of those objects. So you’re unnecessarily using general fixture, so that after each test runs, you and the objects that you don’t need are actually being created. So that will also create a complex test as well as-- it will be slower, and the intention of the test will also be not clear, why you are actually having it. 
Irrelevant information actually, things that are not related to each other. Sometimes you’re testing-- the business logic through the UA layer. If you’re going to test the business logic you [want to SS] have the unit test domain model. You don’t want to go through a UA layer, because the UAs [boundary SS] change a lot. That’s what’s happening in the controller test. You’re testing a controller, you’re testing the UA. In the initial stages, UA will change a lot. It’s going to change the structure of the [ST SS], a lot of things are going to change, the way it’s going to display will change, if it changes all these irrelevant details, will create complex tests.
 Hard-coded test data. Again, hard-coded test data, data duplication. Minus one for illegal primary key for instance to signify-- don’t know why it is there.
Indirect testing is like you’re testing the business logic through the view layer, for instance. Going through several layers to test the particular layer. So the way you impact the changes of one layer will impact the other. 
Long setup code. You want to always minimize the setup code. You want to be really concise and clear when you are doing a setup code. Create minimal objects you need. Otherwise, you end up with a really confusing code. 
Setup duplication. There is actually a force that’s [interplay SS] between setup duplication. You want to minimize the duplication at the same time you want to also minimize this long setup code. You want to share as much as possible across all the tests. At the same time, you want to minimize the amount of objects you want to create. If you create more objects, you can share across more number of tests. Because it’s more generic. But it’s also going to create instances where certain objects are not really required. So, one way to deal with this thing is to have a context. That’s why you have a nested context. You just say, okay, for this scenario, for this context, you can have this setup that I can share across all these tests for this particular nested context. 
Fragile test. Tests just break. For lots of reasons. Tests can fail because of these four different reasons. How do I categorize according to these four reasons? There is something called as root cause analysis. Five Why’s. You keep asking yourself, Why? Why? Why? Why? Why? Why do we have a fragile test? When you encounter a fragile test in your project, if you keep asking yourself the why five times, that root cause can be categorized into these four broad categories. Either it is interface sensitivity, like the UI layer like I was talking about, UI changes a lot. UI is a thing which changes frequently and [I got an account of a new thing SS] and it changes a lot and suddenly everything that you’re testing on, if it’s view test, all the view tests are tightly coupled-- it’s going to break. 
The behavior sensitivity. The behavior is the customer comes up with a new requirement and the marketing comes up with a new requirement and they give you the new requirement, and if you just code the new requirement, the tests are going to break. Because the tests are written for the old requirements. If the behavior changes, test should actually fail because if the code is broken as far as the test is concerned. So the test is the one which is going to actually drive the code change so that the new behavior comes in, the test actually can detect conflicting requirements. If you’re adding in a new feature, if it’s going to conflict with the existing one, your old test will fail. So you want that, being able to identify the conflicting requirements, vague requirements, language changes.
A: To be clear, are you recommending, if requirements do change, you write a test for the new requirement and then go back and fix the old tests? 
S: It’s up to the customer to decide what is valid. This is how our existing system behaves. Are we changing the existing way things behave? Is this going to be overriding us, or do you want us to preserve the old one, because we have a conflicting requirement. Conflicting requirement sometimes it comes through, vague requirements, all these things come through. When you have to actually make it work, that’s when we have this problem. And you have to resolve it; they have to resolve it. It’s not up to me to decide what the customer is going to get in the next [from me SS].
A:I’m saying, do you recommend writing a completely new test for the new requirement or finding the old test and modifying it, or does it really not matter?
S: When you’re going to do test-first programming, when the new requirement comes in, I’m going to write a test.
A: A new test, not just the old-
S: Acceptance test to prove that it doesn’t exist yet. It’s a new behavior. I write this test and then I start using it to write the unit tests and when I’m trying to make the new unit test-- if new unit test fails, because it’s not dead, when the new unit test passes, the old unit test is going to break. Why? Because of the conflict. 
A: So always write a new test?
S: Always start with a test first.
A: What I’ve been doing is modify the old one. 
S: The problem-- if you don’t do test-first programming, you will not reap the benefits of test-first programming. Benefits of test-first programming are minimalist code, minimalistic design, simplicity. That’s what we’re aiming for. No waste actually. There is no waste. You don’t speculate what the customer needs or what they mean. It’s all driven by acceptance criteria that we have agreed with the customer. 
A: [Inaudible 0:12:37]
S: Yeah, because these things happen. It happens, and how will you deal with it? This is how we deal with it.
A: Because we know we have written this code to test this, so we just delete that, or-
S: Yeah, if you just want to go home, that’s what you have to do sometimes. But the thing is, if you want to-- it’s a way to communicate. It’s up to them to actually decide which one is higher priority for them. Satisfying a new customer who is requesting this which is competing with the old customer. Who’s spending more money? Only they know; we don’t know. We cannot make the decision. It’s up to them to tell us which provides them more business value. 
A: You said that-- let’s say if you have a before each and then you’re creating an object and assigning some attributes, some of the tests are not using that-- doesn’t require that object to be initiated.
S: Set a different context; that’s what I was saying.
A: Yes, set a different context.
S: That’s why you have [different context SS] in RSpec. You always want to minimize the amount of objects you need. 
A: So within the context, do the before each and then-
S: Yes. So user is valid, you do this, user is invalid, you do that, but it depends on-- you always want to minimize the context so that your tests [have run faster SS]. And also it becomes clear because those objects are not required to test the thing you’re testing right now. 
So we saw the behavior sensitivity and the data sensitivity. I showed you the back door manipulation some time ago. I went to the database directly, set up the test fixture, and then I exercised the system, and then I went to the back door to test it. Okay, the data sensitivity basically says, If you’re depending on this, and somebody else changes the data, your test is going to get impacted by the change. The change in the data. 
And the last thing is the context sensitivity. I’ve been telling you about minimize the test context. Why? If all the test context, all the tests that require them, it’s okay, you won’t impact. If you have like sharing a large number of graph of objects reusing across all the tests, even if you change just one of those things, lot of tests are going to be brittle. They’re going to break because there is dependency. And it becomes very difficult to actually pinpoint which tests are really impacted by which one, there is no clarity there. So you want to be careful how you actually share the context between the tests. 
So we saw the bad tests, now we’re going to see some good tests. And then we can start discussing  about the stubs and mocks. That probably will be only 10 or 15 minutes away from when we finish that, from doing our Kata. 
So the good tests. So exception test. Let’s say you are writing a test, and your external [0:16:22] service goes down. They throw an exception: service not available. The way it comes when you’re trying to access the browser, but the web service they’re sending you [that] exception. But if you want to really create the overloaded exception on the server, are you going to send like thousands of records to the real server and try to simulate it? It’s too expensive, right? You want more control over certain things. You want to make impossible situations happen, like out of [0:16:55], five system fool, those resources. What happens if somebody unplugs the network cable? Network is down. You can’t really pull down the cable while the test is running. You want to simulate the exception, that’s when you use a stub. Should throw this exception. Should return this exception. You have those APIs to have more control. So you can make your code more robust. You always need to have the exception test. This is a good example for a good test. 
Isolate the system under test. We already saw how we use mocks to isolate. Isolation is a good thing. And we always depend on things which are more abstract than the direction of this pointing thing is always toward something which is more abstract. Having this layer architecture, dependencies are always going down. Somebody replays this layer, it’s isolated, so you isolate the SUT. If you have this well-designed architecture, you can isolate the SUT easily and test them easily in isolation. Benefit of isolation is, it allows you to run the test faster. And it does not depend on the order in which you run the test also. 
Minimize the test overlap, we discussed to some extent how the SUT-- if you have an overlapping SUT, you will have some test which is covering same thing as some other test, and we also discussed when do you delete the test? Even if you have a test overlap, sometimes you want to retain the test based on the value that it provides in terms of communication. Tests should not introduce risk. Like the reason I’ve been saying no if statements, no loops, all those things is because we don’t want any of those logic to introduce risk. Whenever you have complicated things, then that can introduce risks in the code. If you have conditionals, you cannot really assert, Okay, this is the code path that this is going to go through for this particular scenario. It could pick any code path, I don’t know; it depends on the data set.
So reduce the risk. How can you reduce the risk? That’s how you test your own utility, [have SS] your own utility, and you test them separately, and then reuse them across all your different tests once that is well-tested. And you can depend on it. 
Bug repellant is basically defect localization. If there is a bug in the code, I want the corresponding test to fail, so defects are localized. And also, whenever you have bugs coming in, you basically have the regression test, you also ask your code a question. Where else can this kind of thing happen-- similar thing can happen? So that you can actually increase the resistance to the bugs.
A: Do you have any examples for any of these things? I could correlate-- let’s say you mock any marketing feature as a [0:20:32] and then you run it.
S: A simple thing like a half by one item, for instance. So where else do you have this possibility of a half by one item occurring? It’s just a matter of making the code more robust. But it’s such a basic thing to overlook. It’s a good idea to revisit your code and make sure you have coverage for those kind of scenarios. So you missed one boundary condition, for instance. What happens if the customer suddenly downgrades on the 28th of February on a Leap Year. Where else can this kind of error happen? It happens once every 11 years or something. But if it’s so far out you don’t even have to worry, but if it’s going to happen much more frequently, then you need to actually have a test.  Sometimes it’s the general concept, it may be applicable to so many different parts of the system. That’s the part you’ve been working with. Maybe some other team has been working on those things; they need to be aware so they can look into their code and fix it. It’s more like a communication error.
Fully automated. We don’t want any manual intervention. 
Test has to be self-checking. It has to say whether it passed or it failed. Green or red. Just by looking at it, you need to know if some continuation regression server is running, it needs to be able to email you the report.
Repeatable we saw. No matter what the order of the test I run, it should consistently give me the same result set. Every time I run it. It should not depend on the data set I provide. It should be readable. 
And defect localization I already discussed. If something goes wrong in the production code, one aspect of it, I just want one test corresponding to that fail. I don’t want hundreds of tests failing because of that. 
Robust test, it doesn’t really mean robust in the sense-- some people define this robust test as, Test is passing for the right reason and it’s failing for the right reason. No, it doesn’t mean that. This is robust in the sense that test is robust if it fails for one reason and one reason only. If you change something that is not [valid SS], it should not fail. That is the definition of robust test. As defined by {0:23:01], Kent Beck, those guys. [0:23:02] is another guy who defines the robust test as something else, but I don’t consider that as a robust test. It seems to differ based on who is actually defining that.
Focused test, yes, the test should be focused on testing one thing and one thing only, one concern. Don’t test the [view layer SS] and controller layer, and the model layer, everything and then it’s not focused. It’s more like a distracted test. It should be clear. Assertion; I should have included this much before when I introduced the assertion. Fixture, we actually talked. 
External fixture, what does external fixture? Most of the time you’re going to be using libraries like factory girl or something which is going to manage the fixture for you. So it’s more like transparent, as a developer it’s transparent to you. As a developer you don’t manage this thing manually. You only need to know how to use nested context, how to minimize the test-- how to remove the setup duplication, how many objects do you need? You need to create minimal amount of objects that you can share across. 
[Stop video 4 ("TDD bootcamp part 3") at 0:24:18]

