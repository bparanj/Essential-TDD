
Character to Number Conversion

Problem Statement
=========
---------

Convert the character representation of an integer to it's decimal format.

Discussion
=========
---------

Computers can store data in bits (zeroes and ones) which are numbers that can be converted to decimal, octal etc. They cannot store letters or other special symbols. ASCII character encoding allows computers to store letters, text, symbols and control characters.

Solution Domain Analysis
=========
---------

Let's convert 4 character sequence '1984' to the decimal number 1984.

![alt text](http://www.rubyplus.com/images/character-number-conversion.png "Character to Number")

Here is the ASCII table, you can see the decimal representation of the characters '0' to '9'.

![alt text](http://www.rubyplus.com/images/ascii-table.png "ASCII Table")

To convert, 49 needs to be converted to 1000, 57 to 900, 56 to 80 and 52 to 4 units. To get the decimal digit we have to subtract 48 (ASCII value of character '0') from each of the decimal value for the character.

To convert the one-character string to it's decimal representation in ASCII we can use the ord method of String class in Ruby. Playing in the irb:

```ruby
  > '0'.ord
 => 48 
  > '1'.ord
 => 49 
  > '9'.ord
 => 57 
  > '8'.ord
 => 56 
  > '4'.ord
 => 52 
```

First number = 49 - 48 = 1
Second number = 57 - 48 = 9
Third number = 56 - 48 = 8
Fourth number = 52 - 48 = 4

![alt text](http://www.rubyplus.com/images/character-number-conversion2.png "Character to Number")

The shifting to the left mechanism can be obtained at each step by multiplying the previous decimal value by 10 and adding it to current decimal digit.

Assumption
----------

Length of the character 


Test Cases
----------

class CharacterConverter
  
end

describe CharacterConverter do
  it "should convert '0' to 0" 
  it "should convert '1' to 1"
  it "should convert '10' to 10"
  it "should convert '100' to 100"
  it "should convert '1000' to 1000"
end




class CharacterConverter
  def initialize(n)
    @n = n
  end
  
  def to_i
    @n.to_i
  end
end

describe CharacterConverter do
  it "should convert '0' to 0" do
    cc = CharacterConverter.new('0')
    result = cc.to_i
    expect(result).to eq(0)
  end
  
  it "should convert '1' to 1"
  it "should convert '10' to 10"
  it "should convert '100' to 100"
  it "should convert '1000' to 1000"
end

Simplest implementation passes.



  it "should convert '1' to 1" do 
    cc = CharacterConverter.new('1')
    result = cc.to_i
    expect(result).to eq(1)    
  end


Passes


  it "should convert '1984' to 1984" do
    cc = CharacterConverter.new('1984')
    result = cc.to_i
    expect(result).to eq(1984)        
  end

Also passes. We don't want to use Ruby's to_i method which does the conversion. We want to develop our own implementation of to_i. Let's not call the to_i on the String.


  def to_i
    '0'.ord - @n.ord
  end

This passes first test. Add teh second test. It fails.


There is gap in our problem domain analysis. We have not thought about how many digits there are in a given string, so we don't know when to terminate. Reading the Ruby documentation, I found a method that solves that problem for us. Playing in the irb:

```ruby
 > n = '1984'
 > n.bytes
 => [49, 57, 56, 52] 
```
 
 
  it "should convert '10' to 10" do
    cc = CharacterConverter.new('10')
    result = cc.to_i
    expect(result).to eq(10)    
  end



class CharacterConverter
  def initialize(n)
    @n = n
    @numbers = n.bytes
  end
  
  def to_i
    first_element = @numbers.shift
    first_number = (first_element.ord - '0'.ord)

    if @numbers.size > 0    
      next_element = @numbers.shift
      next_number = (next_element.ord - '0'.ord)
      puts 'kjljljlkjlkjlkj'
      first_number * 10 + next_number
    else
      puts 'hi'
      first_number
    end
  end
end

describe CharacterConverter do
  it "should convert '0' to 0" do
    cc = CharacterConverter.new('0')
    result = cc.to_i
    expect(result).to eq(0)
  end
  
  it "should convert '1' to 1" do 
    cc = CharacterConverter.new('1')
    result = cc.to_i
    expect(result).to eq(1)    
  end

  it "should convert '10' to 10" do
    cc = CharacterConverter.new('10')
    result = cc.to_i
    expect(result).to eq(10)    
  end

  
    
  xit "should convert '1984' to 1984" do
    cc = CharacterConverter.new('1984')
    result = cc.to_i
    expect(result).to eq(1984)        
  end
  

  it "should convert '100' to 100"
  it "should convert '1000' to 1000"
end

This implementation passes all three tests. How can you avoid puts in test code? Why use puts when you are writing tests? H

Discuss how to use Diagnostic assertion.


Reference
=========
---------

[Table of ASCII Characters](http://web.cs.mun.ca/~michael/c/ascii-table.html 'ASCII Table')

