Your Database Is Your Friend
Learn how to use your database to make your Ruby on Rails applications rock solid.
by Xavier Shay

Introduction
What is this?

Welcome to the Database Is Your Friend screencast. My name is Xavier, and this is the online version of an in-person course I ran in 2010 about using your database effectively. I've updated small portions since then, but mostly in the ordering, presentation, and auxiliaries here. The core concepts have stayed the same as they have always been since relational databases came into use a few decades ago. Although it is pitched at Ruby on Rails developers, and that is what I use for my examples, the concepts are technology agnostic and applicable to any application that uses a data store. Similarly, while I use PostgreSQL for my demonstrations, the topics covered are applicable to all SQL databases such as MySQL and SQL Server. There is a short appendix on MySQL specific issues at the end. So, if you encounter problems following along, check there first. I'm going to cover three major topics: Data integrity, concurrency, and reporting. The first two will likely be the most immediately useful if you are running a production application already since they directly address issues I have seen in virtually every Rails application I've worked on or heard about. I generally introduce each topic with an exception page or error condition, all of which I've seen in production, and then work backwards from there to an elegant solution. The third reporting module isn't as much of a quick fix, but gives you some concepts to help understand why your current schema may be suboptimal for the things you are tying to do with it and provides some alternative modeling techniques to fix.

Logistics

I don't explain any Rails fundamentals in this course, but the current maps pretty closely to underlying database concepts, so you should be able to follow along even if you are not that familiar with Rails. It should be fairly obvious how this current for instance maps to a create table SQL statement. Since this is a course on databases, I've spent a lot of time in Rails migrations, which are a fancy wrapper around up/down scripts to modify database state. Even though they do support rolling changes forward and backwards, in a development environment I find easier to usually just blow everything away and build the schema again from scratch whenever I want to change it. To do so, you'll see me use this command often, bundle exec rake db:drop db:create db:migrate db:seed. It drops the database, recreates it, applies all the migrations, and then loads in some fake seed data from db/seeds.rb for demonstration. For brevity, I don't always show this being run, but it needs to be whenever a migration is updated. So, if you're following along at home and get stuck, running it is always a safe bet. On following along at home, Source Code is available in the downloadable materials. In particular for the concurrency module, I'd recommend trying out the scenarios yourself and seeing how they play out under different conditions. It's tricky material to get your head around, and trying them out for yourself will help you understand. With that intro out of the way, let's jump straight into the first module on data integrity.

Data Modelling
Introduction

Welcome to this module on Data Modelling. I'm going to be presenting a number of techniques for using your database schema to effectively solve common problems I have seen in Ruby on Rails applications. Each technique is presented as a problem that you're likely to experience or possibly already have experienced in your own deployed app. I will then quickly cover the common fixes and why they are less than ideal followed by a superior solution that uses the power of our database to solve the problem quickly, robustly, and elegantly. By the end of this module, we'll have learned about not null constraints, foreign keys, handling duplicate data with uniqueness validations, polymorphic association alternative, reactive data integrity checks, and how to use all of the above in your own Ruby on Rails applications. Let's get started.

The Unexpected Nil

Here is a typical exception from a Rails application. It's a NoMethodError where we are trying to call truncate on the blurb of a book. We expect the blurb to be a string, and up until now it has been, but something in our app has changed and we are now getting a nil when we didn't expect one. The easiest fix to this problem is to force blurb to be a string. This can be done in the view using the to_s method, which converts a nil to an empty string. This fixes the problem; however, it is a band-aid fix. Since we encountered this problem once where we were assuming that blurb could not be nil, it's very likely that we have the same problem elsewhere in the code or we're likely to make the same mistake in the future and hit another nil error for the same reason and have to make another band-aid fix. Put in other words, what we have done here is patch over the exception locally, but we haven't paid down any debt to prevent our future self from coming across similar errors. I'm going to roll back that change now so we can investigate better ways to do this.

Nil versus Empty String

Let's investigate the data model to see if we can understand what this nil is trying to tell us. In our rails console, I'm going to grab all the books and pull out the id, the title, and the blurb. You can see here that some of these books have blurbs and others have nil values. How could this happen? Editing a book in the UI to remove the blurb works and does not create an exception. And looking at the data, it has set the blurb to an empty string, not a nil. What's the difference between an empty string and a blurb? It could mean "Not provided as distinct from "Provided but empty," but that is exposed in the UI and is not a concept that we care about in this bookstore. This is called incoherent data. It doesn't make sense, it's inconsistent, and it doesn't have any meaning. A nil blurb is a bug in our data model. We need to do two things. One, figure out how the nil got here; and two, make it so this cannot happen again. Step one would normally require some detective work, but I know the history of this system and know that someone created an import script that reads in a text file and creates books based on the title. In this code, we are missing the blurb entry, so this is likely what caused the nil values. As with the to_s fix, we could make a local fix here by adding an empty blurb and fixing the invalid data with a migration. It would still be a band-aid fix though. It would be easy for someone to create a similarly broken import script in the future or other similar code to create a book with this incoherent nil blurb. We can do a better job than that.

ActiveRecord Validation

The first tool you might reach for could be an ActiveRecord validation, something like validate blurb is present. This is not quite accurate, however. We do want to allow empty blurbs. We need to have some extra logic to say only apply this validation if blurb is nil. This is somewhat better. A similar import script in the future won't work, but it's semantically misleading. We don't really want to validate presence on the blurb. In English, that doesn't really make sense. We just want to prevent nils. It's also more added complexity than we would like, especially when you consider the extra code we'd need to write for this, and it's still not a guarantee. Though harder, it's still possible to get a nil into the database. We could use a direct SQL statement or there are plenty of ActiveRecord methods that'll let you bypass validations. For instance, I could create a new Book and then save it using validate: false. The book here now has both a nil, title, and a blurb, so we still haven't quite fixed the problem.

Not Nil Database Migration

The best solution is to fix it in the database itself. We can change our database schema such that it will never allow a nil value to be stored. We're going to do this by using a migration, and migration needs to do two things. First, it needs to update the existing data to make it valid, so we only use update_all for that. The second parameter here is the where clause, blurb is nil, and the first parameter is what to set it to, a blank string. We then change the schema changing the type of the blurb column. It's a text column, and we want to disallow null or nil values. We're also going to set a default value so that if a value is not provided such as a no import script, an empty sting will be used. This can often be a good idea for things like blurb when they're optional. If it was the case where a blurb really was required and we wanted to discourage empty values, you may leave this default off so that it will raise an error if not provided. But for this case, an empty default is good. So, let's run that migration. We go back to the book that was erroring and see that it still works. If we run our import script again, now we have new books, but they don't error out. We haven't changed code at all. We changed our database schema so that nils can exist. I really like this solution because it simplifies my conceptual model of the system. I don't need to think is it a nil, is it not a nil? I don't need to write defensive code around it. I'm allowed to assume correctly that blurb is never nil, which makes me a happy programmer. It's one less thing that I need to worry about. It's also a nice hint to the database. The database can now simplify its searching logic, and you can see performance improvements particularly on large data sets with a schema change like this.

The Unexpected Nil: Recap

To recap, we started off with a nil exception and our first attempt at a fix was a local band-aid. This was the to_s function that we added. Other band-aids you might reach for are conditionals such as unless blurb is nil. So, that was fixing it at display time. We then tried to fix it at create time by adding an empty blurb at import time. This is better than fixing at view time, but we still had a problem in that code that will be written in the future could still have the same problem. We then wrote a validation, but it felt a little weird when we still had edge cases where we could create invalid data such as inserting directly into the database or using validate false. We were also writing extra code and adding complexity to our system to deal with the incoherent data. Our final solution and our best solution was a schema change where we constrained our data model such that it was impossible for nils to ever exist in the system. If we look at the diff, we only have a small migration that fixes the old data and then changes out data types for blurb. And, of course, our schema.rb changed to go with it. It's a very small fix, and we don't need to change any of our logic or any of our assumptions elsewhere in the code. We can continue to assume correctly now that nils will not exist in our system.

The Missing Parent

Let's have a look at a similar, but different issue. In this case, we have an exception that looks very similar to the last one that we had. It's an undefined method name for NilClass. And just like the last problem, we can apply a fairly simple local fix. We can add an if statement to skip over the name if the author does not exist. This fixes the problem, but as with last time it is a local fix, and we're likely to run into the same issue further down the track. Let's have a look at the data to see if we can get a better idea of what's going on here. I'll pull out all the books, their id, their title, and their author_id. If we look here, each book has an author_id, but when we try and find the author, there's actually no author here that exists with this ID. It's not that the books don't have an author specified. It's that they did have an author, and for some reason the author doesn't exist anymore. This is clearly a data integrity problem. There's no valid reason why we would have an author_id of 1 in a book record with that author actually not existing.

ActiveRecord has_many options

Let's see if we can replicate how this issue occurred. I'll return the data to a known good state where the author still exists and our fix hasn't been applied. One thing you can do to an author in the UI is destroy it, so let's destroy this author. The author is now gone, but looking at our list of books, we're getting the same exception. The author has disappeared, but has not deleted any of its dependent books. If we look at the rails log, the delete action shows us the SQL that has been executed, DELETE FROM "authors," but books are not referenced. We have deleted an author that had some child books, and that has caused us a data integrity problem. There is a Rails way to fix this problem. The author model has many books, and has_many takes an option named dependent. You can set the desired behavior on dependent records when the model is destroyed. By setting dependent: :destroy, we can see that all child records are also destroyed when the author is destroyed. We can verify that in the Rails log. The transaction was started, each book was deleted individually, and the author was deleted as a final step. You'll note that each record is deleted individually and not as a group because ActiveRecord using this destroy option will load each record individually and execute their author destroy books. This can be a problem if you have a large number of records. It could take quite a while, so it's worth knowing about. There is another option dependent: :delete_all, which will delete all records in the database and skip callbacks, which is much more efficient, but with the obvious drawback that you don't get any callbacks. Author destroy isn't commonly used though, so this doesn't tend to be a big issue in practice. In this case you might think if we have all of this data in the system for the author, we probably don't want want to remove all the data, or at least we want to warn about it in some way. For this, ActiveRecord provides the restrict_with_exception option. What this does is that if you try to remove an author with child books, it will raise an error saying that it can't be destroyed. There is also a restrict_with_error option that rather than raising an exception will cause the model validation to fail and will add an error message to the object that you can display in your UI. That's a good first step getting ActiveRecord set up correctly, but as with the last problem, it's not that reliable. There are still many ways in which we can break the data integrity of our system. And easy one is to pull out an Author and use delete rather than destroy. This sends a delete directly to the database skipping all callbacks and ActiveRecord settings, which will leave invalid books behind. In this case, for a better solution, I'm going to change our database schema in such a way that prevents this problem from even being possible.

Foreign Keys

We're going to use a tool called a foreign key, a concept that is present in all modern relational databases. A foreign key sets up an explicit relationship between a parent model and the child model enabling the database to take responsibility for making sure that the IDs all match up and that you don't delete records that would leave invalid data in the database. We'll start by using some raw SQL so you can see what this looks like. This is standard SQL syntax. Where we add a foreign key is to ALTER the child TABLE, ADD a CONSTRAINT, and give it a name. It's a foreign key constraint on author_id, so we'll all it that. So far, we've only referenced the child table in the relationship. We then say that our books table REFERENCES authors and the id caller. This sets up a relationship between author_id and id, and I'll show you the effect of that in just a second. To remove the foreign key for our down migration, we use the DROP CONSTRAINT option to ALTER TABLE and pass it the name. So, let's run that. We can now investigate what effect this has on the database. I'll note here that if you're following along at home with SQLite, it does support foreign keys, but it doesn't support adding to existing tables. We have to drop and recreate the table to add the foreign key. MySQL, PostgreSQL, and SQL Server and all the other databases support this syntax. I recommend using one of them for the rest of this course. Let's look at the database. We have both books and authors. If we try and delete from authors, we now get an error from the database itself. Update or delete on table "authors" violates foreign key constraint. Key id is 1, is still referenced from table "books." It's telling us that if we deleted this author, we would be left with invalid data. And so in the Rails application, this exception propagates up protecting us from the case where we've neglected to add our dependent options through has_many or somehow bypassed them.

Cascading Deletes

Preventing the record from being destroyed is the default behavior of a foreign key, which is ON DELETE restrict, but you can also set it to ON DELETE cascade, which has a similar behavior to when we did dependent delete_all in our ActiveRecord model in that when you try to delete an author from the database, it will automatically go through and delete all the child records. This is a fairly dangerous option. You won't see any extra SQL in your logs. It will just say delete from authors and make no mention of books. Also, since we're really setting up this schema to protect us from unintentional errors, we're normally better off setting this cascading delete relationship behavior in ActiveRecord itself. If you want this behavior, I recommend using the default restrict at the database layer and then using delete_all in your ActiveRecord model. This is a little weird because conceptually we want to have ON DELETE cascade, but since we're implementing it at the ActiveRecord layer, we want to make sure that we are preventing it in the database to ensure that everything is going through ActiveRecord.

Schema.rb and Foreigner gem

Another issue we need to consider when using foreign keys is that by default they are not included in the schema.rb file. This is problem because this file is used to populate your test database so when running tests no foreign keys will be created. The code that you were testing won't be the same as the code you were developing and deploying, which is a big problem. There are two ways to fix this. The first is to enable an ActiveRecord configuration option to dump the schema using SQL rather than Ruby. I really like this option. I like dealing with raw SQL. It's much easier to ship around and load onto different databases and other operational things like that. It's not database agnostic, which could be a problem, but I find that on most of the applications I work on, db agnosticity isn't really a big win. Even in the cases where I've had to move from MySQL to PostgreSQL, converting the schema wasn't the biggest problem I had. I tend to use this SQL option by default. It would be nice though if Rails did support foreign keys, and it would also be nice if we could avoid writing raw SQL to generate foreign keys and add migrations. There's a gem called foreigner that adds this behavior. Add it to your gem file, and now we can switch back to the Ruby schema format and see that the foreign keys are included in the schema using add_foreign_key. We can also use that same method call in migrations, and migration now becomes much simpler. The name is automatically generated for us, and so is the down method. Running the migration again, we can see that has indeed added a foreign key.

The Missing Parent: Recap

Let's recap. We started off with a NoMethodError because one of our books had a nil author. Our first attempt at a fix was fixing at display time, which we did by using a conditional, if book.author. We didn't like this because we would prefer to fix the underlying data and prevent other problems from cropping up in the future. Our second attempt at a fix was to fix using ActiveRecord, which we did using the dependent option to has_many. This was good progress, though we still hit cases where we could violate the assumptions that we wanted to make about our data, for instance by directly deleting an author from the database and leaving invalid books. Our final and best solution was to combine what we learned about ActiveRecord with a foreign key at the database level. This provided us the extra protection and guarantees that we were after and made is so that we could offload all the worrying about our data integrity to the database itself. Going forward, we know that all of our books are going to have a valid author.

Duplicate Data

We've just seen two examples of problems where the issue was that we were missing some data. Let's look now at a problem that is caused by too much data. Here you notice that we have two books that have the same title. This is weird because in our ActiveRecord model we've put a uniqueness validation on title so there shouldn't be this case where the two books have the same title. We have an extra problem where if we try to edit a book, we are unable to do so because of a validation on the title rather than the blurb that we were trying to edit. We have invalid data in our database, and this causes all sorts of problems. In the concurrency module, I'm going to cover the more common ways this type of invalid data can come into being, but for the meantime, let's just use what we already know. ActiveRecord validations can be bypassed by inserting data directly into the database. In other words, let's take it as a given that we know how this data got here and move forward trying to fix it. Later on in the course, we'll revisit that given in more detail. The common theme so far has been fixing problems with database migrations, and this problem is no exception. We're going to add a unique key to the title attribute of our book record. Adding a unique constraint is the same as adding an index, only that you're adding an additional option, unique: true. Running this migration, we run into a problem. It would be great if we could put a unique index on our table, but the existing data is already invalid so the database is unable to provide an invariant that the data is unique. What we have to do is fix the data first before we add the index. I'm going to talk about three different methods of doing so.

Resolving Duplicates

The first strategy is arbitrarily deleting one of the records based on latest updated time or some other criteria. Let's revisit our migration and edit it to do exactly that. First, we need to write some SQL to pull out all the duplicate books so we can deal with them. We can use find_by_sql for that, but we'll have to write a tricky aggregate subquery. This is best sketched out using your database's command line client. We'll start by selecting all the titles from the books just to look at our data. You can see the duplicates there. Let's group those by title, and then we want to restrict it to only those with more than one particular title, so just a simple author. We'll then do an outer query to select all the records from the book where a title exists using our inner query, and this is how we pull out all the duplicates from our table. We can then take that query and paste it into our migration. Now that we have the records inside Rails, we can group them by title to give us a hash of title with an array of books. We can then iterate over this data structure, and for each title, we take the books that match it and sort by the created_at date. The oldest will be first. We drop that one off our list. And then for the remainder, we destroy them. This has the effect of keeping the oldest book whilst destroying all the new ones. Running that migration, we see that it applies cleanly and our uniqueness invariant is now enforced by the database. You'll have to choose logic here that makes sense for your application. Another example that might make sense here is choosing the book that has the longest blurb. So, we're assuming that this particular book is the one that's being worked on and one that we want to keep, and we can use the same approach that we did to drop the first line and destroy the rest. So, that's the first strategy, just arbitrarily choosing a book. The second strategy we could do is to automatically merge the duplicate records. So, an example of this, we might want to take all of the books and combine all of their blurbs together. So, we'll pull the blurb off each book and uniq it, and then just joint them together with some new lines. We then set the blurb of the first book, this new blurb that we have generated, and then delete the rest. The same strategy can also be used to merge child records if you end up in that situation. The third strategy is some kind of manual merge where you actually review every duplicate by hand and figure out how you want to resolve the conflict. This is obviously the most expensive option, but if your number of duplicates is low, it can sometimes be a good way of doing it. If you have more than a trivial number of records and you still want to do manual conflict resolution, you might consider building a tool or a web user interface that you can point people at to get them to resolve the conflicts easily.

Retrying Duplicate Exceptions

Now that the migration has applied, we can see that we only have one book with the same title rather than two. What I want to demonstrate now is the protection that the unique index gives us beyond and above the ActiveRecord validation that we already have. I'm going to gloss over all the details of this for now, but we'll come back and revisit it in more depth later in the concurrency module. By adding a short sleep to an after_save in our book model, I'll be able to set up a case where we can actually see the unique index in action. We'll have two sessions going. The first is in a console where I'm going to create a book, and I'm going to save that book and at the same time save a book with the same title in the user interface. And you can see here that an exception has been raised. This is the case where previously we would have got a duplicate book, but now that we have a unique index, it protects us. The DETAIL here tells us that my book already exists. The one that already exists is the one that we created in the background in the rails console. What's interesting is that if I retry this request, this time around the validation kicks in. The first time around it couldn't see the record, and now it can. This suggests that what we should do if we receive that record not unique exception in our code is to retry. We're going to add a new method to book called save_with_retry_on_unique, and this is going to do exactly what we just did in the user interface, but automatically. First, we will try a save. If we get an ActiveRecord NotUnique error, we'll retry the block. We then need to use this method in our controller. Now if we replay the same scenario from before with the rails console in the background, this time around we get a validation error straight away rather than an exception. In production code, I like to make this more robust. This naked retry scares me because it's easy to create an infinite loop. I'm going to add code here so that we only retry once. We'll start by setting up a counter. In the retry block, I'll verify that we haven't retried already before retrying the entire block. Otherwise, I'll just raise the original exception. Going one step further, I would then extract this logic into a generic method that I can use elsewhere in the code. I'll call it retry_on_exception, and we'll pass in the parameters. This simplifies the original save_with_retry method and makes it much clearer what the intent is. Going one step further, I can extract that into a concern so it can be used on any model in the system. I'll create a new one called RetryMethods and paste that code in there. The final code we end with is quite clean.

One-to-one Relationships

It's easy to remember that a uniqueness validation needs to be backed by a unique constraint in the database, but there is another case where you also need to use a unique index that's harder to remember, and that is the case of a has_one relationship. In this example, an author has one profile. The profile has extended attributes like Bio, Born, Died, etc. Looking at the Rails log for this request, everything looks fine. There's a SELECT "authors".* FROM "authors" and a SELECT FROM "profiles," but notice the ORDER BY and LIMIT clause on the profile. This is to show us that we will always get the same profile out. What happens if we have two profiles in the database? Typically, Rails protects us from this problem using this ORDER and LIMIT. But if we do happen to have two profile records for the same author, it will be a lurking problem waiting to bite us. If we try to run the same statistics against our database or provide a report where we're pulling out the name and date of birth of our authors, you'll notice that we get a duplicate entry. This is because when we select from authors there is one record, but selecting from profiles there are two with the second record being invalid. In this case, a unique index on author would have prevented this error from ever occurring. Given that you should have an index on author_id anyway in order to do efficient lookup, making sure that it is also unique is usually pretty straightforward. To add one on top of existing data that may be invalid, you can use all the same techniques from the previous section.

Duplicate Data: Recap

To recap this section on duplicate data, we started by talking about three different strategies for removing invalid duplicates that you might have in your data. The first strategy was arbitrarily choosing one of the duplicates and just deleting the rest. The second was writing an algorithm to automatically merge multiple records. And the third strategy was a manual merge, so either going through it yourself by hand or providing a tool so that others can manually fix up the duplicate data that you have. We then talked about adding a uniqueness index in the database itself so that we won't have to deal with duplicate data in the future. The database will guarantee uniqueness on our behalf. We then discussed how to actually deal with these uniqueness violations in the app when they occur, and the easiest way to do this is by simply retrying the exception assuming that you have your ActiveRecord validation set up correctly. And finally, don't forget about has one relationships. They have all the same uniqueness requirements as a typical validates uniqueness of.

Polymorphic Associations

Let's look now at a trickier dialogue modeling problem. This book store provides a feature for people to comment on books and authors. This is implemented using a polymorphic association. This is a typical Rails technique for associating one type of thing, a comment, with others. In this case, books and authors. We have a supporting controller that can associated a comment with a parent dynamically. In the database, this is implemented by storing both the reference ID and the reference type in the comments table. We learned earlier that whenever we have an association between tables like this, we should use a foreign key to protect against often or invalid records. How do we do that when the reference table is dynamic like this? Without resorting to vendor specific triggers, it turns out that we can't. There is no standard SQL compliant way to do this. Also, this polymorphic data structure is more expensive to index since committable type needs to be included in all the indexes, and in most application, the data distribution tends to be lopsided. For instance, books receiving in order of magnitude more comments than all of this. They also tend to be harder to work with when using raw SQL.

Converting to Separate Tables

Let's look at a more database friendly alternative that can often be used instead. Rather than a single table for all comments, you can use a separate table for each type of comment. You may balk at this, but we can still keep our current really dry, and the payoff of a simple and more reliable data model is usually worth it. Let's start with our migration. Setting up a loop over all of the types of comments we need, the migration doesn't need to be changed that much to create separate tables. We can then also set up foreign key relationships, which was not possible before. Running this, we can now see the two separate tables in our database. We now need stub classes for each type of comment, and we'll include a common comment or concern that can still call our methods. Our parent associations need one small change as well to reference the new class. Finally, we can repurpose our Comment class to act as the backing object for our comments form. It no longer inherits from ActiveRecord since we no longer have a comments table, but we can use it as a pure Ruby object that our controller and view can use in order to create the comments. Those are all the changes we need. The user interface for our application is unchanged. We can still add comments as before. Separate tables gives us a strategy to maintain data integrity and makes indexing and performance easier. The downside is that it is more difficult to operate over all comments, and it is slightly more expensive to add a new comment type. In practice, I've found this tradeoff normally worthwhile, but there are still some cases where a polymorphic setup, the original setup that we had, is the way to go. Usually when you need the same event on lots of different types of things or data integrity is relatively not important or you need the convenience of querying all types as one unit, event logging or an activity fade is a typical example.

Reactive Integrity Tests

In this module, we have covered a number of different techniques for changing your database to easily solve problems. We otherwise would have had to use a more complicated work around worker for the code. We used proactive constraints that prevented problems from ever occurring in the first place. Unfortunately, in practice this is not always possible. Perhaps the constraint is too complicated to be expressed in the database or it's infeasible to add it to an existing system. On one application I worked on, we needed a foreign key for one of our primary relationships, but our large test system was written in such as way that most of it broke when we added it, and it would have been really expensive to fix. In this case, we have a fallback reactive option, integrity tests. This is a test suite that regularly runs against a copy of your production data. This provides a lot of flexibility. The easiest thing we can do with this is to iterate over every model and verify that they are valid. This does not give us a useful error message though. You won't know which model is invalid. I prefer a more sophisticated method where we collect all the invalid records and report them at the end. I also use an early exit so that I get fast feedback on obvious failures. If 50 records are broken, it's more than I can fix by hand anyway, and more data isn't useful. Iterating over ActiveRecord models is comprehensive, but quite inefficient. We can also use SQL statements in our test to quickly check other types of data integrity problems.

Wrapping Up

That's all for this module on data modelling. We covered only a few of the basic techniques such as not null constraints and unique indexes, but these should be sufficient for most problems that you run into. Many databases provide more sophisticated techniques such as check constraints and database triggers, but I tend to avoid them. Rails applications really want to be in Ruby as much as possible, and it's only in cases where the problem is so common and the database solution is so simple and cheap that I reach for database constraints. For example, it's possible using PostgreSQL to set a database constraint on a column to verify that it matches a valid email regex. This is something that I would not use. Accidentally bypassing standard active record validations to put an invalid email in the database is much harder than just getting a nil in there so it's much less common a problem, and it's complex enough logic that I want to be able to develop it in root. On the other side, a not null constraint tends to be fundamental to the data type and does not tend to change over the lifetime of an application, but also really easy to accidentally put into your database, so it's cheap and easy to put it in right at the start so you never have to deal with problems down the track. In addition to not null and unique constraints, we also covered using foreign keys, which let the database maintain referative integrity valid parent-child relationships for us. In order to support adding constraints, it was sometimes necessary or prudent to remodel your schema, and we did this by refactoring a polymorphic association into separate tables so that we could take advantage of our database strengths using it to enforce data integrity and enabling it to query effectively. Unfortunately, it's not always possible to change your database. So to finish, we covered a fallback technique for maintaining data integrity creating a test suite of reactive integrity checks that we can run against a copy of our production data. In the next module on concurrency, we'll start by investigating how the database constraints we've just learned about protect us from data corruption scenarios that it's impossible for Rails to detect.

Concurrency
Introduction

Welcome to this module on Concurrency. Concurrent code is any code that is executing in parallel or at the same time. In Rails applications, this typically maps to multiple web requests being handled at the same time. Under these circumstances, code that works for a single isolated process such as you likely have in development, tends to break in weird and wonderful ways. I'm going to be showing you a few examples of this that I have seen in production Rails applications, as well as a robust solution that make use of your database's features. I will be covering three main techniques: Database constraints, locking, and isolation levels.

A Development Environment for Concurrency

In order to demonstrate the problems and solutions in this section, I'm going to make some changes to our development environment. By default, a Rails development server runs in single-threaded mode making it impossible to replicate any problems that would caused by multiple requests being processed at the same time. First, we'll use unicorn as our server, which forks multiple processes to handle multiple requests. We'll just use a standard configuration for Rails applications that takes care of correctly handling signals and the ActiveRecord connection port. I'm also going to add a monkey patch to the Rails logger that pre-pins every line with the process ID. This will make it easier for us to untangle logs later on. A tricky issue when trying to demonstrate concurrency problems is that they are hard to replicate. You need to time things just right so request to process at exactly the same time. Throughout this module, I'll be adding artificial delays using sleep to make issues easy to replicate, but I want to emphasize that this is only for teaching purposes. In the real world, requests regularly happen at the same time without this need for padding. To show that everything is set up properly, I've added a sleep to our index controller and opened that page in two tabs. Refreshing them both concurrently, then looking at the Rails development log, you can see that their logs are intertwined. Each request was handled at the same time separately by an individual unicorn worker each. Now I want to demonstrate how this causes problems. Remember from the last module that we added a unique index to prevent duplicate titles? Let's revisit that problem.

Concurrent Requests Primer

I've removed the uniqueness constraints from the last section, so now we only have an ActiveRecord validation on title. By adding a sleep to the before_save hook of the book, I introduce a delay between when the uniqueness validation fires and when the new record is inserted. Once again, this is just to make things easy to follow. This same scenario occurs in the wild with no explicit sleep. In the web interface, I'll perform a common user action by double-clicking the Save button on a new book, which generates two requests to the server. From that perspective, it appears everything has worked, but let's look at the logs. We start processing the first request, opening up a transaction, and running a SELECT to search for existing books of which we won't find any. We then hit the before_save hook and pause. While the first request is asleep, the second request starts being processed. It performs a save action selecting for existing books and not finding any. At this stage, both requests think that they have valid books. The second request then sleeps, and the first wakes up again to insert a record and finish the requests. The second request then resumes and does the same thing. We now have two books with the same title that are both invalid. Trying to edit our book in the user interface causes a validation error, and in investigating the rails console we see that yes both books were created in the database despite being invalid. We could add some JavaScript to the form to disable the submit button after click and trying to prevent this second request. This is a good idea for UI reasons, but it doesn't fix the underlying problem. For instance, we can hit the issue making the same request from two different tabs. It's a rarer case, but that only makes it even more confusing when it happens. Adding uniqueness constraint in the database is the only way to guarantee that duplicate records cannot occur.

Concurrency and Unique Constraints

Let's look at the effect adding a unique constraint to our database has on the concurrency of our application. To simulate two simultaneous requests, I have here two database consoles. Obviously, here without the benefit of ActiveRecords validations, it is trivial to insert two records with the same title into the database. I'll remove those duplicates and add a unique index to this table. In each console, I'll start a new transaction. Until now, I've glossed over transactions, but it's important to understand what they are and what they provide. A transaction is an atomic unit of work. All of the statements in a single transaction will either succeed or they will all fail. There'll be nothing in between. They also provide consistency guarantees to prevent the state of the database shifting during the length of the transaction. I'll talk about this more later in the section on isolation levels. We start our transaction with the BEGIN keyword and finish it either with COMMIT to apply all the statements together or ROLLBACK to throw them away. Having opened up a transaction in each session, I'll insert the same record in each now that we have a unique index on the table. The top one appears to succeed, but recall that we haven't committed the transaction yet. This new book hasn't been saved. What's interesting is the bottom session. It is currently blocking. The database knows that someone, the top session, is trying to insert a book with this title, but it doesn't yet know whether it has succeeded, so it is waiting until it finds out. If we COMMIT the top transaction, the bottom is allowed to proceed and hits the uniqueness constraint throwing an error. If we setup the same scenario again, but ROLLBACK the top transaction, the bottom one is able to successfully insert. You can see how a unique constraint prevents duplicate data in our database while still allowing our application to behave correctly.

Concurrency and Foreign Keys

Let's look at one other example of a problem that could be caused by concurrent execution. I have modified the web interface to allow an author to be set for a book and remove the foreign key we added in the last module. In one tab I add a new book while in the other I remove the author. We get a nil error for the author when looking at the book despite having dependent destroy set on our author model. This is the same problem we encountered in the last module. It needs to be solved with a foreign key. Looking at the Rails log, we see the same intertwining of queries, and the book was inserted with an author_id of 1, which is no longer valid. Resetting the database to bring back the deleted author, then jumping to our two database consoles and adding a foreign key, we can set up this concurrent scenario by inserting a book in the top session and removing the author in the bottom. Notice how here again the bottom query is blocking waiting for the result of the first. This is provided by the foreign key. We can roll that back and re-execute the commands in a different order showing that no matter what the ordering, the second one will block and wait for the first to complete.

Thinking Concurrently

Thinking concurrently is hard. For any two pieces of code, any interweaving of them is a possible execution, and we need to guarantee that all of them are correct. The most common error in concurrent code is leaving a gap between a query and an update that depends on that query. In that gap, another process can come along and change the underlying data so that when it comes time to update, the data that the update was based on is now incorrect. We saw this when checking for uniqueness of a book's title. There is a gap between the SELECT to look for other matching books and the INSERT of that book. In between the two, a second process came along and inserted a matching book. The assumption that the first INSERT was based on, that there were no other books with the same title, is now violated, and invalid data ends up in the database. In the previous two examples, database constraints have provided some protection against this problem, but they are not always applicable to solving concurrent issues. Let's look at another two techniques to close the gap.

Optimistic Locking Introduction

Let's look at a problem known as a lost update. I made it in the same book in both tabs, pretend these are two different users, and we are updating it with different details. When we both press save, one set of changes is lost. Now, depending on your application, this might not be that big of an issue, but if I've spent an hour crafting the perfect blurb, I'm not going to be too happy to lose all that work. In this case, there is a gap between when we start editing a book and when we save changes during which someone can change the book from under us. We will blindly clobber those changes. One solution to this issue is a technique known as optimistic locking. We add an extra where clause to our update to validate that the record we are updating is the same one that we started with. This closes the gap by duplicating the initial query into the update fusing them into one statement. An individual SQL statement is always atomic. The second query will now fail rather than overwrite changes allowing us to resolve the conflict. In SQL, what this looks like is looking at the blurb, making our changes, then making sure the blurb has not changed when we update it. This statement returns the number of rows that were changed, either 0 or 1, which allows us to detect whether our update succeeded. If we try this update again, you can see that it fails because the underlying data is now not what we expect it to be.

Optimistic Locking with ActiveRecord

ActiveRecord natively supports a coarser-grained method where it uses a single integer column, the Lock_version. Let's add that column in a migration. Trying to do an update now in the rails console, you can see that rather than querying individual columns that have been edited, the SQL only checks that the Lock_version is the same before incrementing it to a new version. Trying to update a book that no longer has the right Lock_version raises an error. To use this in our UI, we need to store the current Lock_version in a hidden_field that's included as part of our update. The most expensive part of an optimistic locking solution is conflict resolution. We need to deal with this exception. The lock is optimistic in that it expects everything to be fine, that there won't be any conflicts. It is optimized for this case, but at the cost of us having to manually resolve conflicts. For a simple UI like this, we could present the edit form alongside the current version of the book and ask the user to reconfirm their changes. We need to update the Lock_version on the book that we are editing to reflect the fact that we are showing the user the most recent version of the book. If we don't do this, any further updates will continue to fail.

Counters

Using Lock_version and providing a conflict resolution UI is overkill for many apps, but there are more common smaller lost update problems that need to be dealt with in a similar way by closing this gap. Here is another incarnation of a lost update. I've rolled back our Lock_version change from before and added an In Stock counter column to books with buttons to increment and decrement it. Using naïve Ruby code to do this, namely the ActiveRecord built-in increment and decrement methods, we can spot a gap in the generated SQL. We query for the number of books, subtract 1 from it, then update it with the new number. What happens is two of these processes interleave? Each process will query 10, subtract 1, and both processes will update the stock to 9. Our value should be 8 though. We've lost an update. Optimistic locking would prevent this, but there is a simpler solution for this case. The spirit of optimistic locking is to combine our query and update into one single statement. For incrementing and decrementing counters, this is straightforward. Rails provides two methods, increment_counter and decrement_counter, which generate a single atomic SQL query that updates the given column in line in an UPDATE statement. Now we don't need to worry about our counters getting out of sync.

Optimistic Locking Recap

We've just looked at a technique called optimistic locking. It's optimistic because it works best when the common case is no conflict. It closes the gap by combining a query, an update, into a single SQL statement, which is always executed atomically. There are some downsides. If conflicts are common, it can block people from updating. Imagine repeatedly trying to save your update only to find that every time someone else had come along and changed the data out from under you. It also relies on the application to respect the locks. Misbehaved code can just ignore Locked_version and clog your updates. Let's look now at a more heavyweight solution.

Pessimistic Locking

Optimistic locking techniques push logic down into the database, but sometimes that isn't easy to do, particularly when using third-party code. I've added the aasm gem, acts as state machine, to demonstrate this type of problem. Acts as state machine allows us to set up different states for a book with transitions between them and triggered actions on those transitions. Here I want a notification to go out when a book is discontinued. For today, it just writes to the Rails logger, but in a real system it would send an email or some other external notification. We run into issues when two processes try to transition the book at the same time. There is a gap between the initial query to get the state and the update that transitions that state. As a result, we get the discontinued notification twice, which is incorrect. If we were to apply an optimistic locking solution to this problem, we would add on a WHERE clause to the transition to verify that we were actually in the expected initial state. That code happens down in the bowels of acts as state machine though, which I'm not keen to modify. Instead, we are going to use a technique called pessimistic locking. This closes the gap by instructing the database to not let anybody change the record we are interested in. This is done by using the FOR UPDATE suffix on our initial query. This will lock the record for the duration of our transaction. You can see here that the bottom UPDATE is blocked waiting for the top transaction to complete. The only way to release a lock is to complete the transaction, either with a COMMIT or a ROLLBACK. SELECT FOR UPDATE will block if the record is already locked. Once it returns, you are guaranteed that no one else can change that record until you are done with it. ActiveRecord provides a lock method that is the equivalent of reloading the object with the FOR UPDATE suffix. I'll add a sleep in here to make the solution that I demonstrate in a moment clearer. If we run that before our state transition, including both in a transaction, we can no longer get a double update. Pessimistic locking has helped us successfully work around some concurrency issues in acts as state machine. The same technique could be applied to most third-party code.

Pessimistic Locking Recap

Our pessimistic lock is so called because it assumes that a conflict is going to occur. It is the most robust solution. Since the locks are done at the database level, there is no way for misbehaved code to bypass it. It is the most expensive and complicated though. It takes longer to develop, there are some tricky edge cases to deal with such as deadlocks, and they make the database do more work as well. Only use this technique if you really need to. In general, that is when either contention is high or the cost of obtaining the lock is less than the cost of resolving a failed update. Even in both of those cases you can usually do conceptual pessimistic locking at the application layer rather than the database, which I'll talk about in the next section.

Concurrency in the Real World

Optimistic and pessimistic locking are concepts we can apply to other concurrent scenarios outside of the database. Let's look at some real world examples. Git, a version control system, is optimistic in nature. I can edit any file I like, make any changes, and it's only when I try to push my changes to a remote server does it verify that nothing has changed or reject the save is something has. It then provides a rich suite of tools for resolving the conflict such as merge and rebase. An older version control system, Visual SourceSafe, takes a pessimistic approach. It requires an explicit check out or lock of a file before you can edit it. In this way, it prevents a concurrent editing of files so it doesn't have to deal as much with resolving conflicts. If you look around, you'll start noticing concurrent systems and resolution algorithms everywhere, queuing at the DMV, overbooking on flights, traffic lights. Examples abound in the real world. We're going to tackle a specific problem in our bookstore. A popular new book is being released, and we only have a limited number of copies. We are announcing it online, but people will need to come to the store to pick it up in person. Let's try applying both optimistic and pessimistic strategies to see what that would look like. Using an optimistic strategy, we push our synchronization to the very last minute and assume there won't be any conflicts. A customer sees the announcement on the website that the book is available, jumps on their bike, cycles down to the store, and tries to pick up the book. In this case, a conflict means that there are no books left by the time they arrive. The customer has wasted their time coming down to the store, and their expectations have been violated. This is a terrible experience. They won't be a happy customer. Not only is the cost of a conflict really high, but the probability of one is high too since this is a popular book with limited stock and an announced availability time. What does a pessimistic strategy look like? When the customer sees the book is available, they can reserve a copy. They can then take their time to cycle on down to the shop without fear of someone else taking it. We have a happy customer. We would also add a timeout to put the book back in the pool if they do not pick it up in a normal amount of time, which we can inform them of upfront. In database terms, this is a rollback. This is a much nicer user experience. There is still a small chance of conflict at the beginning of the process where we show a reserve button that may be clicked on only to find there are no books left to reserve, but that's a far smaller cost than what we paid in the optimistic scenario where the customer rode all the way down to the store only to find that no books were left.

Combined Locking Example

In building this reservation system, we want to use a pessimistic locking strategy, but we are unable to use database locking across web requests since they are bounded by a single transaction. Instead, we are going to build our own application level pessimistic locking system. We'll start by supporting the first two actions, reserve and pickup, and then we'll add abandon and some other rollback cases at the end. We'll use the existing in_stock column on book, and we'll add a new reservations table with a foreign key to books. We use the primary key of this reservation as a ticket number we give to the customer in order to claim their book. In practice, you'd want to use something less predictable. As long as our reservation row exists, we consider that to be a lock held on one copy of the book. To reserve a book, we need to create a reservation record, but first we need to verify that there is unreserved stock remaining for that book. A naïve Ruby solution would check the field on the book; then if it succeeds, create a reservation and decrement the counter. This solution is broken in two ways. See if you can see them before moving on. The first problem is that creating a book and decrementing stock are not atomic. It is possible for the first to happen and then the process to crash and leave our stock number out of sync. To fix this, we need to wrap the two statements in a transaction signalling to the database that this is an atomic unit of work that either both segments or none of them should be applied. The second problem is that there is a gap between our query and our update. In between the first check of in_stock and when we update it, it is possible for another process to decrement the value, which could result in a negative stock number overall. We've just covered two methods of closing this gap. We could explicitly lock the book model or we could combine the query and the update into a single SQL statement. I'm going to do the latter since it's easier on the database. First, let's convert this update to use update_all, which will make it easier to adapt to our needs. It's now doing our update. We can fold the query into it as well. Recall that this statement will return the number of rows updated. So, if the number of rolls updated is 1, then we were successful and proceed with creating a reservation. That should be sufficient for our reserve method. Unfortunately, it's a little more complicated than the naïve Ruby version, but it's much more correct. Looking at the generated SQL, you can see that in the success case both statements are inside a transaction and the stock decrement is done atomically in a single UPDATE statement. In the failure case, no rows are updated or created, and the method returns nil. To pick up a book, we simply need to destroy the existing reservation. No change is needed in the stock counter since it was already decremented when we initially reserved the book. The destroy command will raise an exception if the reservation doesn't exist, and the least surprising behavior here is to catch that exception and swallow it. I've probably double-clicked a button in the UI, and I've definitely expressed an intent to abandon the reservation. I don't care that it's already been abandoned. Now we have a minimal reservation system that we can put a UI around. I'm not going to do that here since it's pretty straightforward. Instead, let's add some more intelligent behavior to the system. Rather than picking up a book, a customer could choose to explicitly abandon it. This is similar behavior to pickup, except we also need to increment our stock counter. We can use increment_counter here since we don't need an extra where condition. And since we have two operations that need to act as a unit, we wrap both of them in a transaction. Now let's consider a trickier situation. How do we automatically abandon reservations that were not picked up in a reasonable amount of time? We set an expires at value when the reservation is created. The easiest method is to treat that value as a recommendation, and then just clean up expired reservations in a background loop with from another recurring task schedule. The code for that is rather straightforward. We loop over all expired reservations and abandon them. If we hit a RecordNotFound error say because somebody had already abandoned this reservation by the time we got to it, then we just skip over it. And interesting exercise is to treat expires at as exact, and expire reservations only when we fail to create one. In other words, in the else block to this if statement, you clean up any expired reservations, then retry acquiring a reservation if any were cleaned up. Note that we can't just create a reservation directly if we cleaned one up since that would create a gap in which another process could snap up that reservation we just freed. It's important to go back through the lock process. Let's look at the final SQL generated by this method. I'll manually expire a reservation so we don't need to wait a day to continue the screen cast. A book has one unit left in stock and one expired reservation. The first call to reserve grabs the final remaining book using the same SQL we saw earlier. The second reserve is more complicated. It opens a transaction and tries to reserve a book, but finds none available. This triggers an expiration loop, which finds and destroys our expired reservation and increments the stock counter. We then retry the whole method and are able to reserve the freed-up book. You might be asking yourself why use an explicit counter that could be out of sync with the number of reservation records? Why don't we just use a count (*) in SQL? The answer is somewhat complicated, but it's exactly what I'll be covering in the next section.

Acts as List

Until now, we have only dealt with concurrency issues with single records. Both of the optimistic and pessimistic strategies we looked at locked an individual book record. Handling concurrency across a range of records requires different techniques. To demonstrate this, I'll add the popular acts_as_list gem that enables a list of records to be stored in any arbitrary order. It does this by using a position column on a model. We'll just globally sort our books model for this example. I've added up and down links to the index page so that we have a reordering UI with control methods that just delegate to the acts_as_list methods. To enable me to easily demonstrate the problem with this setup, I'll introduce a short sleep again in the books after save callback. Remember from earlier that this is only for demonstration purposes, and this effect can be seen in the wild without the sleep. By sending two requests to the server and having them execute in parallel, I'm able to introduce a data consistency issue. Notice that our ordering now has duplicates in it. This corruption worsens over time introducing unexpected behavior where records don't move around as you would expect them to. Let's look at the SQL to figure out why this is occurring. Both requests are performing updates over a set of records, and there are multiple queries here with plenty of gaps between them. Your first instinct might be to just lock all the records, but you'll quickly find scenarios that break that model as well. What about if someone destroys a book at the same time that I am moving one? You can see here that it has left a gap in our positions. The reason for this is here. Moving a book to the bottom of the list performs a SELECT to determine which is the bottom position. In deleting the bottom record, we invalidate this calculation, but the move continues anyway leaving a gap in our ordering. The fundamental problem here is that for our transactions to be correct, they need to operate on a consistent set of data. As it stands, we have records shifting from underneath us, which makes things very difficult to reason about. The SQL standard refers to this issue as that of "Isolation Level," and it provides an option called Serializable that assists with this problem. I'm going to introduce a gem transaction_isolation that enables us to change this setting temporarily in code. By setting our isolation_level to serializable, the database will run the transactions as if they were being run in serial. In other words, guaranteeing that the data that we are operating on will not shift out from under us. Of course, that isn't always possible. So, this introduces another failure mode where a transaction can fail because it cannot be serialized, in which case you probably just want to retry it. This is exactly the behavior we need to solve our access list problem. We can wrap our list operations in a transaction and set the serializable isolation_level. If we get a serialize exception, we can retry. Clicking through our initial scenario again, you can see that our data corruption problems have disappeared.

Isolation Levels

Let's take a step back and look at isolation levels more generally. SQL standard defines four isolation levels, serializable being one of them, that determine what, if anything, a transaction can see from other transactions executing at the same time. Concretely, if I select a book in one transaction, change it in another, then select it again, isolation levels specify whether I see the update or not. The four standard isolation levels are Read Uncommitted, Read Committed, Repeatable Read, and the one we've already met, Serializable. I'll work through each in turn. The implementations for these differ across databases, so I'll use MySQL to demonstrate the first half, then PostgreSQL for the rest. I'll cover why this is so later on. Read Uncommitted specifies that whenever you select the record, you will see the most recent version of that record even if that version hasn't been committed yet. This is known as a dirty read. If we select the title of the book, change it in another transaction, but do not commit, then read it again, we see the new value. For operational logic, this is almost never what you want, but in some databases this level can give performance improvements for reporting queries where you don't particularly care about individual records and are just trying to get a feel for the overall table. Read Committed is a step up from Read Uncommitted. It prevents the dirty read that we just saw and only allows you to see committed versions of other records. We can repeat the previous example with this new isolation level of Read Committed. We can see that the uncommitted value is not seen. Once we commit that value, we can now read it. This is a different type of behavior now called a non-repeatable read. We read a value at the start of a transaction, and then we weren't able to repeat that read later on because we got a different value. As you can probably guess from the name, the next isolation level of Repeatable Read prevents non-repeatable reads. Again, repeating the previous example, but with this new isolation level, we can see that the read is now repeatable. During this transaction, we will only ever see the one value. For individual records, this is as isolated as you can get, but there's another type of contaminating result that is allowed under a Repeatable Read. It's called a phantom read. This happens when you select multiple records, then insert a new record. Under Repeatable Read or lower, this new record will appear. The final level, Serializable, prevents phantom reads as well. Repeating the insert, this time we see the same count no matter what. This table summarizes the difference between the four levels. Read Uncommitted is the only level that allows a dirty read reading uncommitted data. Read Committed prevents this, but we'll see any newly committed updates, which could be different from values already read, a non-repeatable read. Repeatable Read prevents seeing updates, but still allows seeing inserts, what's known as a phantom read. Serializable is fully isolated and prevents all of these scenarios. Different databases provide different default values for their isolation levels. Read Committed is a standard, though MySQL is the exception. The different databases also support different isolation levels. MySQL supports all four, PostgreSQL only supports Read Committed and Serializable, and SQL Server has a couple of extra ones more fine-grained than the standard levels. An important caveat about isolation levels is that they specify minimum guarantees. This accounts for the discrepancies between databases. In PostgreSQL, if you ask for Read Uncommitted, you'll actually get Read Committed, which is within spec since it's a higher level guarantee. MySQL's Repeatable Read is in practice much closer to Serializable. In particular, it actually prevents phantom reads. You'll have to read the documentation and run your own experience for your database to see how it in particular behaves. Why wouldn't you just run the Serializable level all the time? One reason is that in general lower levels are more performant and allow for greater concurrency. You can make sense of this intuitively. Less overhead is required if I can just show any transaction-able version of a row whether or not it is being committed. And if Serializable is acting as if all transactions are serialized, meaning they are applied one at a time to the database, then concurrent throughput is going to be lower. Database technology is pretty advanced these days though, so make sure you run benchmarks before making any performance-related decisions. There are also more failure scenarios the higher the isolation level such as serialization failures in PostgreSQL or deadlock errors in MySQL, which you'll have to handle in your application. The isolation level you choose may also affect the operational characteristics of your database. For instance, in MySQL, you cannot use Read Committed if you are using statement-level replication. You need to switch to row-level replication to enable it. So, once again, make sure you read the documentation for your specific database to see if the isolation levels affects what you can do with it.

Summary

There are a couple of key takeaways from this module. Writing concurrent code effectively is all about defensively closing gaps that open up in your code. Any time there is a gap between a query and an action on that query, there is the potential for a concurrency-related bug. We covered a number of tools and techniques for attacking this problem. First is simply good application of the data integrity techniques covered in the last module such as foreign key and unique constraints. They catch many potential issues and prevent them from every corrupting your data. When constraints are not sufficient, we can use either optimistic locking where we close the gap by combining the query and update into a single atomic action or pessimistic locking where we defend the gap by putting up barriers to prevent any modifications to our query. Both of these techniques can be done at the database level, but those primitive database operations can also be combined to create higher level locks at the application level, which we used to build a book reservation system. Finally, for dealing with sets of data rather than just single records, we covered isolation levels and how they let you tweak the rules your database uses for showing or hiding concurrently executing transactions from each other. Up until now we have been dealing with operational systems, systems that care about what is happening right now. Coming up in the next module, we're going to cover a new way of modeling data to support reporting systems that need to query back in time.

Reporting
What is reporting?

Welcome to this module on Reporting. I'll be covering a different type of problem in this module. We launched our bookstore to an enthusiastic market, books are selling, and now we want to understand how the business is operating, how it is trending. We've widened our scope from individual orders to orders over the last six months in the last year, and we're asking questions about where we should focus our business attention. Which books are most popular? Which authors? Which states are buying the most books? Do different authors do better in different states? These types of questions are common across all businesses, but unfortunately the way we model our transactional system is not well suited to answer them. Let's look at why by examining four different questions we could ask of our data. I've added a couple of extra of models to our domain, but it should look familiar. A customer places an order for one or more books, which are entered as line items on an order. An order has a shipping address, and an order can be cancelled. Let's take a simple question. How many of each book are we selling? To answer this question, we need to join two tables, line items for the quantities of books and orders so that we can filter out cancelled ones. I've created a simple rake task to use for demonstrating these reporting queries with a helper function to format results in a nice table. Let's write this report, "Total book sales." We need to SELECT out the book_id and quantity FROM order_line_items, and then we actually want the SUM of all quantities grouped by book. That will give us a basic report, but it still includes cancelled orders. To exclude those, we need to JOIN to orders and add a WHERE clause. Now the report is correct. The SQL isn't too complicated, but it is easy to forget to join in orders to filter out the cancelled ones. We'll come back to this query, but first let's look at some other examples and look for patterns. What if we needed revenue this year broken down by month? This time we need the same set of tables, but we are asking a different question. Rather than quantity, we are looking for revenue, which is the total amount paid for the books. To group by month, we need an extra field, placed_at, from the orders table, and we can use a PostgreSQL specific function call to pull out the month. Revenue is the SUM of the unit_price and the quantity, and then we change our GROUP BY and ORDER BY clauses to be on month. And, of course, don't forget to exclude cancelled orders. Once again, not too complicated, but it was a little fiddly. The third question, revenue this year by author, requires us to pull in an extra model, Book. We'll include both revenue and quantity, keep the placed_at and cancelled_at restrictions from last time, and join in the extra books model to get access to author_id. Our GROUP and ORDER BY clauses change to match. This is starting to get a little trickier. We have to remember where all of the data is, in which relationships, and how to join them in and performance doing as queries is starting to get a little harder since we're joining in so many of them. The final question adds an extra dimension to the previous one, "Sales by author and state." We need to pull in yet another table, which we do with another INNER JOIN. (Typing) We are still asking fairly basic questions of our data, and already the complexity of our queries is getting high. It's easy to forget all of the conditions such as filtering out cancelled orders, and the more conditions you have, the more likely you are to end up with two reports on your CEO's desk, which numbers don't reconcile. The queries are difficult to tune especially because they tend to use columns that wouldn't typically need an index in the operational system, and they erect a high barrier to entry for exploring the data since you need to know where all the pieces are. In the next section, we'll look at what all of these questions have in common and introduce a new parallel data model that is optimized for answering them.

Introduction to Star Schemas

Ninety percent of all reporting queries can be expressed in a simple form. Give me a metric filtered by some criteria, then grouped on a dimension. Applying this to the reports we have just seen, the "Quantity sold by Book" query fits. The metric is quantity, the group is Book, and there is no need for a filter. Revenue this year by month once again fits nicely with both filter and group being on data types. "Revenue this year by author" is the same data, but only grouping along a different dimension. "Revenue by author and state" groups along two dimensions, but it still fits the model. We can use this knowledge to model our data differently in a way that is optimized to answer questions of this form: Metric, Filter, and Group. The scale that we have been using is optimized for answering the question what is happening now? Things such as showing a book, creating a new order, showing books by author, these are all things that support a customer interacting with the system in the present. In contrast, our reports are more interested in answering the question what has already happened? To do this, we are going to use a modeling technique known as a star schema. It is organized around a central fact table representing a specific thing that happened that you care about such as the sale of a book. That fact will have metrics such as how much revenue it earned, how many were sold, and it will also have a number of dimensions that the fact can be grouped by. In our bookstore example, the fact is a sale, which will be a 1:1 mapping to order line items. Then our dimensions are Book, Author, and State. Year, month, and other date types are also dimensions, but they are a little more complicated to handle, so I'm going to postpone covering them until later. You can imagine another fact we might be interested in such as page views. In this case, the fact is the page view itself, which may or may not have any metrics, perhaps time spent on page, and then a number of dimensions we can group those views by. A key principle of star schemas is that they are flat. In our sale example, author_id could actually be derived by joining book_id to books and fetching it from there, but instead we de-normalize it onto the fact record. This is important to keep our querying setting.

Creating a Star Schema

A star schema may appear simple, but it is a powerful technique. It doesn't replace your normal operational model. It augments it. Let's see how it simplifies the questions we started this module with. Often you would keep your star schema in a separate database, maybe even a separate database technology specialized in storing star schemas, but for now I'll just keep it in a separate table. Our fact is going to be called sales, and it has a timestamp and three metrics: Revenue, unit_price, and quantity. Even though one of these is redundant, revenue can be calculated from unit_price and quantity, that is okay in the context of a star schema. We want to frontload any calculations, particularly business critical ones so that there is minimum chance of error when using them. This is especially important when the calculation is more complicated than a simple multiplication. We should decide what our standard is for each business metric and implement that logic once in our transform code rather than duplicating it in every report. This isn't always straightforward, particularly in an established business. Everyone already has their pet definition of revenue, and you have to wrangle that into one or maybe two well-defined metrics that can confidently be compared across the business. Our sale fact also has a number of dimensions on which it can be grouped: By order, by book, by author, by buyer, by state. These dimensions are typically a foreign key to another table containing all the dimensions in that category. For most of these dimensions that is easy since they map directly to an existing table in our operational schema. We'll cheat on state for now since it is just a two letter code, but if we wanted to associate it with an expanded name or other metadata later on, we would split it out into its own table. You would typically expect quite a large number of dimensions for a fact, even up to 10, 50, 100 dimensions for complicated data models.

Using a Star Schema

With this schema in place, let's populate it. There are many strategies for this, and we'll take the easiest, but slowest one. We are going to iterate over all of orders and create sales records for each. At scale, you might consider doing this with raw SQL, some sort of parallelization, or keeping it up to date by reading off an asynchronous message bus or using a specialized ATL tool. It will all depend on your data. This basic technique will get you a long way though, and you can just rebuild your model every night. In this create_from_order method, we can encapsulate all the business rules we want our reports to follow. Right up front we can exclude cancelled orders. We never have to worry about that again. We can copy all the dimensions we need and include our revenue calculation. Running this populates our sales table from some fake order data I have loaded into the database. It's in db/seeds.rb if you're playing along at home. We can now revisit our earlier questions and compare the SQL we wrote then to the SQL that we are now enabled to write against this new schema. For our first question, "Total book sales," we are able to remove both the INNER JOIN and the WHERE clause. The new query is simple and more directly expresses the essence of our question without incidental and error-prone boilerplate. Running this new query beside the old one, we can see that the results are identical. On our second query, once again we can remove JOIN and cancel that condition. We also used the pre-generated revenue column to simplify our SELECT clause. We still haven't addressed the month calculation though. That will come in a later section. The same story repeats itself in our third and fourth questions. No joins, no remembered clauses, and no duplicate calculations. The SQL we can now write is much easier and simpler and has the nice side effect that it is much easier to put an exploratory UI in front of it if you wanted to expose something high level than SQL. Running all four reports, old versus new, we can see that they are all the same. Our new sales table gives us the same answers with simpler queries.

Calendar Tables

Until now, we have been ignoring the time dimension and just using database functions to do our time-related calculations. This goes against our strategy of precalculating relevant business values once ahead of time. A common solution to the time dimension is to use a calendar table. A calendar table is a pre-generated list of all time buckets your facts fall into, which you can then join against. For our model, we're going to use a bucket size of one day, but buckets of 1 hour or a couple of minutes are also common choices. For each bucket, we'll compute all the interesting properties such as year, month, order, day of week, whether or not it's a business day. In the end, we can use it to treat time the same as any other dimension. We are going to perform three steps: Creating the right schema, populating it, then retrofitting our existing reports. First, we create a migration, then create a new table, caldendar_days. I'll use a selection of properties both counters such as year, quarter, day of week, and also Boolean's such as whether or not this is a weekday. Your business will likely have other important properties that you'd want to add here. For instance, maybe which financial year the day belongs to. Rather than introduce an order-generating surrogate ID for the primary key, we're going to define our PRIMARY KEY as the day column. ActiveRecord support for non-ID primary keys isn't very good, but for our purposes, we'll be mostly using SQL and not ActiveRecord. We also need to add a dimension column to sales. Currently, we only have a timestamp. We need to change that to a date that can be a foreign key into our calendar table, the dimension. On is the conventional Rails suffix for date columns. To populate this table, we'll use a rake task, populate_calendar_days. It iterates over all days for some large range of years, and on each day creates an entry into the calendar table. Most of the calculations are straightforward and can be read directly off the data object such as year, month, day_of_month, and week. Others are more complicated such as business_day, which will vary depending on your business and location. For most businesses, weekends are not business days. And, in addition, I'll include here a couple of common U.S. holidays. You can use whatever logic is necessary here. The two most common types are specific dates such as for New Years and Christmas and then a certain day of the month. Martin Luther King Day is the third Monday in January, and Thanksgiving is the fourth Thursday of November. Both are easy to implement in Ruby. Let's run this task and have a look at the generated data. You can see the repeating cycles of day_of_week, and that Martin Luther King Day, the third Monday of January, is marked as not being a business day. This will make our reporting queries much simpler and more consistent. Let's go back to one of our queries, revenue by month this year. If we join in calendar_days, we can simplify both the SELECT and the WHERE clauses down to individual columns. This is much easier to remember and to get right. Running this new query versus the old one, you can see they yield the same results. Joining to the calendar table is going to be one of the most common operations you do for reporting queries, so let's make it even easier again. We can create a database view that pre-joins sales with calendar days. This has the same performance characteristics as joining in line, but hides the complexity from the user. Having added this view to our migration, we can simplify our query even further. Once again, running this versus the old query yields the same results. The calendar table has made our reporting queries even easier.

Multi-valued Dimensions: Creating

So far, all of our dimensions have been many-to-one such that each fact only had one reference to each dimension. This is very neat and easily queryable, but unfortunately the real world isn't usually so kind. Often your data model will have many-to-many relationships, which we need a way to deal with. Let's add a genres model to our bookstore. Each book can be associated with 0 or more genres. This requires a migration in which we'll use all the good techniques we learned in the data integrity module, not null constraints and foreign keys. We also need to set up some ActiveRecord models and associations. (Typing) For our seed data, I'll just create a small set of popular genres then randomly assign them to the books as they are created. Let's run that and verify that our data looks as we expect. Genres are present, and the join table has been populated. How do we now alter our sales fact table to accommodate these genres? This is known as a multi-valued dimension. At first glance, we can't simply add a genre_id column since each book potentially belongs to many genres. It may be possible to do something similar though. Does a book have a primary genre? If that distinction makes sense, reducing your multi-valued dimension to a single dimension, primary genre, is often a good strategy since it keeps querying really simple for users while not throwing away too much useful information. Remember, a good reporting schema exists to enable a wide variety of ad hoc reports in an easy to use manner, not necessarily to contain every single piece of data. There is a tradeoff here that you will have to evaluate in the context of your own business. Let's assume for now though that we can't make a distinction between genres, that we do in fact want to be able to query on all of them classifying a book. The standard technique for this is to use a bridge table. We introduce a concept of genre_group, which is a unique set of genres that a particular book has. We insert this group into a new table, one row for each genre, but sharing a unique key that can be referenced in the fact table. There is a uniqueness constraint across the key and the genre. A genre can only appear once in a group, and if our key is deterministic for a particular set of groups, then we also get a uniqueness constraint across that as well. In our sale model when creating facts, we can generate a group key from the ID's of all genres a sold book has. It's then easy enough to create the group on demand if it does not already exist. Unlike we did for the calendar table, we generally don't want to pre-generate all potential groups since there is a combinatory explosion, and the number of rows gets large very quickly. In practice, usually only a small subset of combinations will be present in your data set. With our genre group created, we can store a foreign key to it on the fact. The reason we don't just use a simple join model between sales and genres like we would in a typical transactional schema is for two reasons. One, a bridge table reduces the number of rows we need to store because repeated genre groups can reuse the same key. This is important when the number of facts in your table gets quite large. Number two, because we want to explicitly call out the dimension on the fact table. This makes it easier for users and tools alike to understand the schema and all the data that is available to them.

Multi-valued Dimensions: Querying

Let's consider some queries we can make with our new schema. We can filter by genre by looking in genre groups and putting a WHERE clause on genre_id. We need to be careful when using the bridge table to aggregate though since it becomes very easy to double count items. For example, let's extend the last query to show revenue by all genres, not just one. For comparison, I'll add one more report here as well for total revenue. Running this, you can see that revenue by genre doesn't add up to the total. The problem is that each book is contributing the full revenue from its sale to every genre that it's classified as. This might actually be fine. If we are trying to get a feel for popular genres, we may not care that the totals don't add up. This type of report is called an Impact report, and as long as they are clearly labeled so people don't expect them to add up, they can be very useful. The alternative report, which will add up to the total, is a Contribution report. For this, we need to figure out business roles for portioning or dividing up the sale of the book among its different genres. The easiest way is to split it evenly. So, if a book in both the comedy and sci-fi genres sales for $10.00, then it contributes $5.00 each to the comedy and sci-fi genre's revenue respectively. To do this, you can add a multiply column to the genre_groups table. When we create a group, we also include a fractional multiplier depending on how many genres are in the group. Note the weighting here doesn't have to be even across all genres. If you have extra information, you can assign the weights differently as long as they all add up to one. Our report can now apply the multiplier and the resultant report sums to the total. Of course, you'll want to round those numbers before you present them to the board. There is a judgment call to be made here. In both the impact and contribution reports, we are effectively making out numbers that have only an arbitrary relationship to reality. Any reports based on this need to be clearly labeled and understood by those using them. It may just be better to not use them at all and instead only use the bridge tables for filtering. Even if you do this, keep in mind that if you filter by more than one condition, for instance books in both comedy or sci-fi, you'll have to remember to apply a distinct filter to your rows before aggregating them; otherwise, you could end up with duplicates. Multi-valued dimensions are a tricky business. They complicate your model and make it harder for end users to understand. That doesn't mean you shouldn't use them, but always remember that in your reporting database you are providing a product for users to extract information about their business, and that product needs to be both friendly and useful. More features aren't always better, and there are many halfway solutions such as picking a primary genre that may be sufficient.

Columnar Databases

As your data volume grows, you may start investigating specialized reporting databases. These are typically columnar databases that differ conceptually from row-based relational databases that you would be using for your transactional system. A row-based transactional system is optimized for individual lookup. You generally care about specific row values and are pulling out individual ones one at a time. For instance, displaying a view page for a book needs to grab the book and all the records. They also need to be able to record individual records fast such as when placing an order so that we can return a confirmation to the user. As such, all the data query row is stored close to one another so they can be accessed efficiently. In a reporting database, we typically care about only one or two columns, but across a large set of rows. Our example queries from earlier such as revenue by book only use the revenue and book_id columns. In addition, many of these columns will have low cardinality, meaning that there are normally many more facts than dimensions, so dimension values for facts will be repeated a lot. This pattern can be supported well by storing data on a per column basis meaning the database can easily access a large number of values for a particular column and also achieve good compression rates to store that data effectively. These reporting concepts are also useful when using a large MapReduce-based store such as Hulu. The important thing to keep in mind is that whenever you are transforming data to make it more amenable to reporting, and I've repeated this a couple of times now, you are creating a product. You need to provide the data in a way that lets you get accurate answers fast. Thinking in terms of facts and dimensions is a tried and tested way to do so.

Summary

There is an entire field and industry around building reporting schemas, and we've only scratched the surface here. There is much to learn about more advanced topics such as Snowflake designs, outrigger tables, fact-less tables, slowly changing dimensions, and many more. What I've covered here are the basic concepts and techniques that should be enough to get you started. Star schemas with their separation of facts and dimensions and their flattening of your data model provide a user-friendly way to answer most types of reporting queries. Give me a metric filtered and aggregated by a particular criteria. They are a selective de-normalization of your transactional schema. We also went over some simple ways of constructing the schema, and also how to deal with some common, but trickier cases. We used a calendar table to enable time to be treated just like any other dimension and a bridge table to handle multi-valued dimensions where a fact can be tagged by more than one value for a particular dimension such as the genre and books. We've made it to the end of this screencast. Between the modules on data integrity, concurrency, and reporting, I hope you've picked up some useful techniques you can use in your existing applications or for any new ones that you build. Feel free to drop me a line on Twitter or by email through my website with any feedback or questions. Applying these concepts to my own applications has made me a much happier and more productive programmer, and I hope it does the same for you.

Appendix: MySQL
Introduction

While I used PostgreSQL throughout the screencast, I know many people out there are using MySQL. While the former excels in its teaching environment, the latter is present in many production deployments including all the large ones I have personally worked on. This appendix covers some MySQL specific issues that are particularly useful to understanding problems you might see in production. I remember how confused I was when I first encountered some of these problems, and I hope to help others avoid it. It contains some explanations and debugging techniques for concurrent scenarios, which are roughly transferrable to other databases, so this may still be of interest even if you don't use MySQL day-to-day.

Handling Invalid Data

By default, MySQL allows invalid data in your database. This can be frustrating, especially if you were trying to replicate some of the scenarios in the integrity module. For example, here I've modified the books table to have an artificially short varchar length of 10 just so I can demonstrate that if I insert a book with a title that is too long, the insert succeeds. The client indicates that there was a warning, which tells us that the value was truncated, and selecting from the table, we can see that this is exactly what happened. This is bad. We have thrown away data. And if this had happened in a server process, we wouldn't have seen the warning. Some database adaptors such as JDBC will error out on warnings, but most do not. We also lose data with other types such as integer values, which have climbed to max if you exceed their bounds or even worse in some other versions where they would overflow and give you negative numbers. And also with datetimes that can end up with really invalid values like all zeros. Thankfully we can change this behavior using the SQL mode server configuration variable. There are many knobs to turn that define how MySQL behaves in response to different types of queries, which you can read about in the documentation, but the one that covers our issues best is TRADITIONAL. This will error on invalid values rather than 1, which we can see if we retry some of the earlier queries. It can be set either in your session or globally for the database in your data column. I recommend setting this value unless you have a really good reason not to.

InnoDB Locking

The most befuddling errors I've had to deal with on a MySQL system were lock wait timeout or deadlock exceptions I received in an application that didn't do any locking. How is that even possible? It turns out that locks are how InnoDB manages its isolation levels preventing transactions from seeing data they're not supposed to. And in addition to the explicit locks you can set such as the FOR UPDATE we used in the concurrency module, InnoDB also implicitly sets locks for certain queries, which is how you can end up in a deadlock or timeout situation. There are two types of locks, exclusive write locks, which is what the FOR UPDATE clause places, although it is also placed implicitly by an update query. There are also shared read locks, which can be held by many people at the same time, but prevent rights. It's rare that you will place one of these explicitly, but certain common statements placed in implicitly such as when you trigger a foreign key check for a row say by changing a parent of a child row. That's a lot of information to process, so let's try to see this occurring in a console. First we start a transaction and try to update the publisher_id of a book. There is a foreign key from publisher_id to the publisher's table. In another console, we can use the diagnostic command show engine innodb status to get information about currently held locks. It looks a lot like a dog's breakfast, but there are only a couple of things here we are interested in. First, notice that there is a transaction open that is holding two RECORD LOCKS. The first is on a record in the books table where you can tell from the X that it is an exclusive lock. This has been implicitly placed by our update statement. We can confirm this by trying to update that same book in another transaction and noticing that our query hangs. It is waiting for the lock. I can cancel that with a Control+C. The other record log is on the publishers table, and from the S we know it is a shared read lock. This is preventing writes or deletes on both the old and the new publishing records, which we can see by actually trying to delete them. If you step back, you can intuitively see why this is important. Since the transaction has not been committed yet, we don't know whether the book is going to belong to the old or the new publisher yet. If we delete the old one, we can't guarantee that the database is in a consistent state if the transaction rolls back, and if we delete the new one, we can't guarantee that if it commits, so the database prevents either until the updating transaction has completed. You may have also noticed the two TABLE LOCKS in the diagnostic output. In practice, they tend to be less interesting, but you can read up on them in the InnoDB documentation. It contains a really good section about what the different types of locks do, when they are applied, and why including the ones we just covered. I highly recommend reading it.

Deadlocks

So, going back to our original problem, how do we get a deadlock without locking? Let's say we are keeping a cache of the books count on each publisher, which is a common Rails idiom. When moving a book from one publisher to another, we need to decrement the counter for the old publisher and increment it for the new one. Each increment will implicitly place a lock on that record. If we move a book from publisher A to B at the same time we move another book from B to A, we could deadlock in between locking each publisher. The first transaction is waiting for the second to release its lock on B while the second is waiting for the first to release A. This lock can't be resolved. Both transactions are blocked, so the database rolls one of them back. You can get information on a deadlock after the fact using the same show engine innodb status command we used to investigate locks. It shows you which transactions will hold in which locks, what they are waiting for, and which one got rolled back. In the case of a deadlock, one transaction will be auto rolled back enabling the other one to proceed. It is nondeterministic which one will be aborted. The general solution is to always update records in a deterministic order inside a transaction such as by using the lowest ID first. It's not only records. If you're updating more than one table in a transaction, you also need to do that in a deterministic fashion as well. Any time you're requiring locks, you always need to acquire them in the same order. Typically in a transactional system you'll be updating single records in various tables, but there are scenarios where you need to do a more complex update over either a range or records or joining in other tables. In these situations, it's very difficult to actually set a deterministic ordering for your locks since your database engine could rearrange the tables or scan ranges in a nondeterministic fashion. Thankfully these cases are rare, and the good thing about deadlocks is you can just retry them if they occur. So, if you are expecting them, make sure that you have the appropriate rescue and retry logic in your code.

Scan Locks

The other implicit locking scenario that is likely to occur is when updating a set of records, probably in a background thread. When doing a bulk update, it's obvious that MySQL needs to place a write lock on the rows that it updates. What is less obvious is that it also needs to lock records that it scans but doesn't update. We can see this in action by issuing a query to fix up some known invalid data. A couple of our books have ended up with negative stock numbers, and we want to reset them to 0. We don't have an index on in_stock, so this query needs to scan then entire table to find matching records. Intuitively, this feels okay. It might take a while if the number of records is high, but it's only going to update a couple of rarely used records. Unfortunately, since we are running under the MySQL default of repeatable read isolation level, innoDB needs to guarantee that any record we read is not going to change if we happen to read it again. This means it needs to block all writes to the books table until the update completes, not just the change records. You can see here both updates and inserts are being blocked. As you can imagine, this is pretty bad if it happens on a production system. There are two solutions to this problem, reduce the number of rows scanned either by using an index or paging the query or drop down to the recommitted isolation level in which scan records are allowed to be updated so InnoDB doesn't have to place the locks.

Summary

In this appendix, we covered some MySQL specific settings and diagnostic tools, as well as studied some more concrete examples of potential concurrent problems. While the details will be different, these same types of problems can occur on other databases as well, and similar debugging approaches apply. Read the documentation, reduce the problem to its simplest form, and have a good hard think.



